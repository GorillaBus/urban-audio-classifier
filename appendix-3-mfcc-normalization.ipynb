{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix #3: Normalized MFCC vs default MFCC\n",
    "\n",
    "<br/>\n",
    "by Eduardo Garcia Rajo @ 2019<br/>\n",
    "<br/>\n",
    "This notebook if part of my project \"Urban sounds classification with Covnolutional Neural Networks\" on my Githubat: https://github.com/GorillaBus/urban-audio-classifier.<br/>\n",
    "<br/>\n",
    "Licensed under the GNU LESSER GENERAL PUBLIC LICENSE Version 3, 29 June 2007<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the average amplitude in audio files seems to be a very common normalization process in speech recognition. Even being an audiophile or having some previous experience working with audio in other contexts it seems very intuitive to normalize audio volumes of the whole dataset before working with it.<br/>\n",
    "\n",
    "#### Amplitude difference\n",
    "The same sound source to be recorded with different amplitude levels depending on facts like recording settings, microphone characteristics, distance from the sensor to the source, etc.<br/>\n",
    "\n",
    "#### Hypothesis\n",
    "When normalizing the average amplitude of every sample in our dataset to the same value we are taking all those values to the same scale. If we consider that amplitude differences as some sort of noise that may take the model to associate certain amplitude ranges to certain categories, normalizing amplitudes would supposedly help in the task of recognizing the patterns that really describe each category.<br/>\n",
    "\n",
    "#### The test\n",
    "But noise is not always bad, sometimes what we consider noise ends up helping to genralize better. We want to do an explicit test where we compare how a model perform when trained with normalized and un-normalized average amplitude to resolve if we may normalize this feature or if we may also use amplitude alteration \n",
    "intentionally to augment data.\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import IPython\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime \n",
    "\n",
    "from keras import backend as keras_backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SpatialDropout2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define general variables/settings\n",
    "\n",
    "# Set your path to the dataset\n",
    "us8k_path = os.path.abspath('./UrbanSound8K')\n",
    "metadata_path = os.path.join(us8k_path, 'metadata/UrbanSound8K.csv')\n",
    "\n",
    "# Ensure \"channel last\" data format on Keras\n",
    "keras_backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata from the generated CSV\n",
    "metadata = pd.read_csv(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads MFCC data from the STANDARD audio dataset (no audio normalizations)\n",
    "# This version (v3) padding is done with NaN instead of zero (v1)\n",
    "\n",
    "num_samples = len(metadata)\n",
    "\n",
    "# Original\n",
    "X_orig = np.load(\"data/X-v1.npy\")[0:num_samples]\n",
    "y_orig = np.load(\"data/y-v1.npy\")[0:num_samples]\n",
    "\n",
    "# Normalized\n",
    "X_norm = np.load(\"data/X-v4.npy\")[0:num_samples]\n",
    "y_norm = np.load(\"data/y-v4.npy\")[0:num_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC nofmralization by centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'axis' is an invalid keyword to ufunc 'subtract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-4bf833367250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Substract mean and divide vy std on non NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX_nan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'axis' is an invalid keyword to ufunc 'subtract'"
     ]
    }
   ],
   "source": [
    "# Compute GLOBAL mean and std dev\n",
    "mean = np.nanmean(X_norm)\n",
    "std = np.nanstd(X_norm)\n",
    "\n",
    "# Substract mean and divide vy std on non NaN\n",
    "X_norm[~np.isnan(X_norm)] -= mean\n",
    "X_norm[~np.isnan(X_norm)] /= std\n",
    "\n",
    "# Convert padding NaN to zeros\n",
    "X_norm[np.isnan(X_norm)] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize indexes\n",
    "def randomize(total):\n",
    "    indexes = list(range(0, total))\n",
    "    \n",
    "    # Randomize indexes\n",
    "    random.shuffle(indexes)\n",
    "\n",
    "    # Divide the indexes into Train and Test\n",
    "    test_split_pct = 20\n",
    "    split_offset = math.floor(test_split_pct * total / 100)\n",
    "\n",
    "    # Split the metadata\n",
    "    test_split_idx = indexes[0:split_offset]\n",
    "    train_split_idx = indexes[split_offset:total]\n",
    "    test_meta = metadata.iloc[test_split_idx]\n",
    "    train_meta = metadata.iloc[train_split_idx]\n",
    "    \n",
    "    return [test_split_idx, train_split_idx, test_meta, train_meta]\n",
    "\n",
    "\n",
    "# Split data\n",
    "le = LabelEncoder()\n",
    "def split(X, y, splits):\n",
    "    # Split the features the with the same indexes\n",
    "    X_test = np.take(X, splits[0], axis=0)\n",
    "    y_test = np.take(y, splits[0], axis=0)\n",
    "    X_train = np.take(X, splits[1], axis=0)\n",
    "    y_train = np.take(y, splits[1], axis=0)\n",
    "    \n",
    "    # One-Hot\n",
    "    y_test_encoded = to_categorical(le.fit_transform(y_test))\n",
    "    y_train_encoded = to_categorical(le.fit_transform(y_train))\n",
    "\n",
    "    # Reshape to fit the network input (channel last!)\n",
    "    X_train = X_train.reshape(X_train.shape[0], num_rows, num_columns, num_channels)\n",
    "    X_test = X_test.reshape(X_test.shape[0], num_rows, num_columns, num_channels)\n",
    "    \n",
    "    return [X_train, y_train_encoded, X_test, y_test_encoded]\n",
    "\n",
    "\n",
    "# Create model\n",
    "def create_model(input_shape, num_labels):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Conv 1\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), input_shape=input_shape, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(0.23))\n",
    "\n",
    "    # Conv 2\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(0.23))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Conv 3\n",
    "    #model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(SpatialDropout2D(0.23))\n",
    "    #model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Conv 4\n",
    "    #model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(SpatialDropout2D(0.23))\n",
    "\n",
    "    # Reduces each h√ów feature map to a single number by taking the average of all h,w values.\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    # Softmax output\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    adam = Adam(lr=1.5e-3, beta_1=0.99, beta_2=0.999)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy'], \n",
    "        optimizer=adam)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, num_epochs, batch_size):\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=num_epochs, \n",
    "                        validation_data=(X_test, y_test),\n",
    "                        verbose=1)\n",
    "\n",
    "    # Evaluate on train set\n",
    "    train_score = model.evaluate(X_test, y_test, verbose=1)\n",
    "    train_loss = train_score[0]\n",
    "    train_acc = 100 * train_score[1]\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_score = model.evaluate(X_train, y_train, verbose=1)\n",
    "    test_loss = test_score[0]\n",
    "    test_acc = 100 * test_score[1]\n",
    "\n",
    "    # Train / Test difference\n",
    "    loss_diff = round(abs(train_loss - test_loss), 3)\n",
    "    acc_diff = round(abs(train_acc - test_acc), 3)\n",
    "\n",
    "    return {\n",
    "        'history': history, \n",
    "        'train_loss': train_loss, \n",
    "        'test_loss': test_loss, \n",
    "        'train_acc': train_acc, \n",
    "        'test_acc': test_acc, \n",
    "        'loss_diff': loss_diff, \n",
    "        'acc_diff': acc_diff\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training competition\n",
    "\n",
    "* On each round I'm  shuffling the indexes of the data and creating two train/test splits using the same index orders with the two different data sources: original and normalized amplitudes.\n",
    "\n",
    "* Models are re-defined at the start of each round (they are never re-used).\n",
    "\n",
    "* A model is marked as 'winner' for a round in which it presents:\n",
    "    * a difference between Train/Test loss that is lower than 5.5% (a quite permeable value before considering overfitted)\n",
    "    * a lower test loss value than it's opponent\n",
    "\n",
    "* Finally we save from the best performing version of each data sources:\n",
    "    * the model itself\n",
    "    * its model history\n",
    "\n",
    "* Default settings: 10 rounds of 20 epochs per model\n",
    "\n",
    "* A per-round metrics registry is saved as a Pandas dataframe for further analysis\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 22:08:57.285316 139817466201920 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0724 22:08:57.424214 139817466201920 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0724 22:08:57.470604 139817466201920 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Round # 1\n",
      ">> Training with STANDARD data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 22:08:57.570349 139817466201920 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0724 22:08:57.571722 139817466201920 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0724 22:08:59.067036 139817466201920 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0724 22:08:59.187792 139817466201920 deprecation.py:506] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0724 22:08:59.320455 139817466201920 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0724 22:08:59.352619 139817466201920 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0724 22:08:59.628680 139817466201920 deprecation.py:323] From /home/eduugr/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 27s 4ms/step - loss: 2.2516 - acc: 0.1925 - val_loss: 1.9964 - val_acc: 0.2623\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.9734 - acc: 0.3003 - val_loss: 1.8171 - val_acc: 0.3459\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.8839 - acc: 0.3317 - val_loss: 1.7334 - val_acc: 0.3780\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.8249 - acc: 0.3455 - val_loss: 1.6830 - val_acc: 0.4044\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7677 - acc: 0.3676 - val_loss: 1.6428 - val_acc: 0.4444\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7386 - acc: 0.3845 - val_loss: 1.6080 - val_acc: 0.4519\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 24s 3ms/step - loss: 1.7010 - acc: 0.3974 - val_loss: 1.5789 - val_acc: 0.4507\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6783 - acc: 0.4012 - val_loss: 1.5526 - val_acc: 0.4559\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6560 - acc: 0.4057 - val_loss: 1.5335 - val_acc: 0.4599\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6310 - acc: 0.4151 - val_loss: 1.5144 - val_acc: 0.4731\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6163 - acc: 0.4231 - val_loss: 1.4943 - val_acc: 0.4759\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6034 - acc: 0.4254 - val_loss: 1.4830 - val_acc: 0.4857\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5759 - acc: 0.4303 - val_loss: 1.4800 - val_acc: 0.4857\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5672 - acc: 0.4383 - val_loss: 1.4720 - val_acc: 0.4868\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5528 - acc: 0.4346 - val_loss: 1.4583 - val_acc: 0.4914\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5367 - acc: 0.4367 - val_loss: 1.4546 - val_acc: 0.4920\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5261 - acc: 0.4485 - val_loss: 1.4576 - val_acc: 0.4800\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5156 - acc: 0.4479 - val_loss: 1.4719 - val_acc: 0.4696\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4949 - acc: 0.4505 - val_loss: 1.4903 - val_acc: 0.4616\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4795 - acc: 0.4572 - val_loss: 1.5336 - val_acc: 0.4473\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 7s 1ms/step\n",
      ">> * New best model, loss: 0.02\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 25s 4ms/step - loss: 2.3660 - acc: 0.1549 - val_loss: 2.0376 - val_acc: 0.2749\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 2.0152 - acc: 0.2292 - val_loss: 1.9754 - val_acc: 0.2239\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9164 - acc: 0.2681 - val_loss: 1.9773 - val_acc: 0.2371\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8520 - acc: 0.2836 - val_loss: 1.8885 - val_acc: 0.2595\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.8194 - acc: 0.2976 - val_loss: 1.8142 - val_acc: 0.3219\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7719 - acc: 0.3239 - val_loss: 1.7650 - val_acc: 0.3276\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7329 - acc: 0.3408 - val_loss: 1.7269 - val_acc: 0.3345\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7115 - acc: 0.3506 - val_loss: 1.7089 - val_acc: 0.3425\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6952 - acc: 0.3571 - val_loss: 1.7109 - val_acc: 0.3477\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6646 - acc: 0.3644 - val_loss: 1.7263 - val_acc: 0.3345\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6481 - acc: 0.3810 - val_loss: 1.7417 - val_acc: 0.3282\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6311 - acc: 0.3841 - val_loss: 1.7568 - val_acc: 0.3179\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6210 - acc: 0.3829 - val_loss: 1.7500 - val_acc: 0.3568\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6060 - acc: 0.3944 - val_loss: 1.7172 - val_acc: 0.3889\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 24s 3ms/step - loss: 1.5988 - acc: 0.3968 - val_loss: 1.6798 - val_acc: 0.4112\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 24s 3ms/step - loss: 1.5887 - acc: 0.3949 - val_loss: 1.6391 - val_acc: 0.4278\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5722 - acc: 0.4200 - val_loss: 1.5948 - val_acc: 0.4381\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5601 - acc: 0.4206 - val_loss: 1.5418 - val_acc: 0.4588\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5529 - acc: 0.4343 - val_loss: 1.5203 - val_acc: 0.4714\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5412 - acc: 0.4342 - val_loss: 1.4889 - val_acc: 0.4811\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> * New best model, loss: 0.009\n",
      ">> Round winner: normalized\n",
      ">>> Finished training tests\n",
      ">>> Round # 2\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 25s 4ms/step - loss: 2.1687 - acc: 0.2217 - val_loss: 1.9314 - val_acc: 0.3162\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9370 - acc: 0.3076 - val_loss: 1.8059 - val_acc: 0.3391\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8337 - acc: 0.3451 - val_loss: 1.7246 - val_acc: 0.3866\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.7871 - acc: 0.3557 - val_loss: 1.6768 - val_acc: 0.4227\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7424 - acc: 0.3806 - val_loss: 1.6323 - val_acc: 0.4444\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6999 - acc: 0.3949 - val_loss: 1.5971 - val_acc: 0.4582\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6673 - acc: 0.4097 - val_loss: 1.5689 - val_acc: 0.4622\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6447 - acc: 0.4123 - val_loss: 1.5455 - val_acc: 0.4679\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6174 - acc: 0.4147 - val_loss: 1.5242 - val_acc: 0.4765\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5991 - acc: 0.4233 - val_loss: 1.5278 - val_acc: 0.4662\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5786 - acc: 0.4248 - val_loss: 1.5295 - val_acc: 0.4570\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5541 - acc: 0.4419 - val_loss: 1.5267 - val_acc: 0.4565\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5403 - acc: 0.4399 - val_loss: 1.4939 - val_acc: 0.4714\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5192 - acc: 0.4425 - val_loss: 1.5070 - val_acc: 0.4530\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4894 - acc: 0.4476 - val_loss: 1.4906 - val_acc: 0.4605\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4784 - acc: 0.4542 - val_loss: 1.4903 - val_acc: 0.4605\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4628 - acc: 0.4608 - val_loss: 1.4623 - val_acc: 0.4633\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4545 - acc: 0.4689 - val_loss: 1.4831 - val_acc: 0.4559\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4272 - acc: 0.4853 - val_loss: 1.4292 - val_acc: 0.4702\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4214 - acc: 0.4790 - val_loss: 1.4165 - val_acc: 0.4834\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 25s 4ms/step - loss: 2.3973 - acc: 0.1592 - val_loss: 2.0161 - val_acc: 0.1695\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9964 - acc: 0.2406 - val_loss: 1.8370 - val_acc: 0.2812\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9314 - acc: 0.2508 - val_loss: 1.8100 - val_acc: 0.3081\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8746 - acc: 0.2708 - val_loss: 1.8502 - val_acc: 0.2658\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8299 - acc: 0.2897 - val_loss: 1.8948 - val_acc: 0.2629\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.7884 - acc: 0.3046 - val_loss: 1.8896 - val_acc: 0.2446\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7554 - acc: 0.3222 - val_loss: 1.8402 - val_acc: 0.2228\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7365 - acc: 0.3274 - val_loss: 1.8098 - val_acc: 0.2617\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7034 - acc: 0.3448 - val_loss: 1.7972 - val_acc: 0.3024\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6897 - acc: 0.3544 - val_loss: 1.7779 - val_acc: 0.3368\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6710 - acc: 0.3709 - val_loss: 1.7633 - val_acc: 0.3482\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6434 - acc: 0.3755 - val_loss: 1.7079 - val_acc: 0.3723\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6358 - acc: 0.3868 - val_loss: 1.6325 - val_acc: 0.4198\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6191 - acc: 0.3915 - val_loss: 1.5732 - val_acc: 0.4622\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6077 - acc: 0.4005 - val_loss: 1.5426 - val_acc: 0.4754\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5948 - acc: 0.4094 - val_loss: 1.5342 - val_acc: 0.4834\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5935 - acc: 0.4153 - val_loss: 1.5289 - val_acc: 0.4811\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5733 - acc: 0.4230 - val_loss: 1.5381 - val_acc: 0.4777\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5670 - acc: 0.4270 - val_loss: 1.5067 - val_acc: 0.4805\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5473 - acc: 0.4294 - val_loss: 1.4721 - val_acc: 0.4943\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> * New best model, loss: 0.007\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 3\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 25s 4ms/step - loss: 2.2558 - acc: 0.1950 - val_loss: 1.9905 - val_acc: 0.2984\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9947 - acc: 0.2804 - val_loss: 1.8444 - val_acc: 0.3545\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9079 - acc: 0.3245 - val_loss: 1.7669 - val_acc: 0.3877\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8344 - acc: 0.3481 - val_loss: 1.7021 - val_acc: 0.4198\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7865 - acc: 0.3609 - val_loss: 1.6615 - val_acc: 0.4221\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7303 - acc: 0.3859 - val_loss: 1.6353 - val_acc: 0.4192\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6865 - acc: 0.3906 - val_loss: 1.6189 - val_acc: 0.4187\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6479 - acc: 0.4044 - val_loss: 1.6077 - val_acc: 0.4192\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6188 - acc: 0.4018 - val_loss: 1.6032 - val_acc: 0.4135\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5879 - acc: 0.4163 - val_loss: 1.5790 - val_acc: 0.4267\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5635 - acc: 0.4243 - val_loss: 1.5575 - val_acc: 0.4278\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5365 - acc: 0.4346 - val_loss: 1.4974 - val_acc: 0.4467\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5151 - acc: 0.4449 - val_loss: 1.4889 - val_acc: 0.4450\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4910 - acc: 0.4499 - val_loss: 1.4757 - val_acc: 0.4605\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4764 - acc: 0.4551 - val_loss: 1.4632 - val_acc: 0.4553\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4669 - acc: 0.4558 - val_loss: 1.4579 - val_acc: 0.4576\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4407 - acc: 0.4674 - val_loss: 1.4483 - val_acc: 0.4645\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4324 - acc: 0.4755 - val_loss: 1.4245 - val_acc: 0.4834\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4204 - acc: 0.4774 - val_loss: 1.4352 - val_acc: 0.4719\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4050 - acc: 0.4867 - val_loss: 1.3528 - val_acc: 0.5080\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 7s 1ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 25s 4ms/step - loss: 2.2733 - acc: 0.1796 - val_loss: 1.9434 - val_acc: 0.2491\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9778 - acc: 0.2547 - val_loss: 1.8832 - val_acc: 0.2583\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8830 - acc: 0.2817 - val_loss: 1.9004 - val_acc: 0.3047\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8296 - acc: 0.2886 - val_loss: 1.8593 - val_acc: 0.3219\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7846 - acc: 0.3205 - val_loss: 1.7678 - val_acc: 0.3528\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7521 - acc: 0.3297 - val_loss: 1.6997 - val_acc: 0.3654\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7199 - acc: 0.3435 - val_loss: 1.6608 - val_acc: 0.3832\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6952 - acc: 0.3570 - val_loss: 1.6547 - val_acc: 0.3763\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6794 - acc: 0.3601 - val_loss: 1.6511 - val_acc: 0.3757\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6586 - acc: 0.3778 - val_loss: 1.6342 - val_acc: 0.3889\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6313 - acc: 0.3833 - val_loss: 1.6154 - val_acc: 0.4084\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6223 - acc: 0.3885 - val_loss: 1.6362 - val_acc: 0.3923\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6188 - acc: 0.3979 - val_loss: 1.6541 - val_acc: 0.3975\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5980 - acc: 0.4077 - val_loss: 1.6105 - val_acc: 0.4175\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5797 - acc: 0.4140 - val_loss: 1.5554 - val_acc: 0.4525\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5765 - acc: 0.4173 - val_loss: 1.5298 - val_acc: 0.4628\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5607 - acc: 0.4320 - val_loss: 1.5026 - val_acc: 0.4748\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5539 - acc: 0.4224 - val_loss: 1.5032 - val_acc: 0.4857\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5487 - acc: 0.4307 - val_loss: 1.4943 - val_acc: 0.4966\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5431 - acc: 0.4357 - val_loss: 1.4849 - val_acc: 0.4971\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 7s 1ms/step\n",
      ">> * New best model, loss: 0.002\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 4\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 25s 4ms/step - loss: 2.1669 - acc: 0.2204 - val_loss: 1.8749 - val_acc: 0.3442\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9188 - acc: 0.3033 - val_loss: 1.8038 - val_acc: 0.3637\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.8171 - acc: 0.3448 - val_loss: 1.7243 - val_acc: 0.3975\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.7533 - acc: 0.3689 - val_loss: 1.6567 - val_acc: 0.4141\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6931 - acc: 0.3833 - val_loss: 1.6146 - val_acc: 0.4313\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6538 - acc: 0.4022 - val_loss: 1.5677 - val_acc: 0.4427\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6151 - acc: 0.4007 - val_loss: 1.5221 - val_acc: 0.4628\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5856 - acc: 0.4180 - val_loss: 1.4739 - val_acc: 0.4731\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5597 - acc: 0.4273 - val_loss: 1.4507 - val_acc: 0.4782\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5380 - acc: 0.4276 - val_loss: 1.4338 - val_acc: 0.4885\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5253 - acc: 0.4372 - val_loss: 1.4247 - val_acc: 0.4914\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5131 - acc: 0.4329 - val_loss: 1.4024 - val_acc: 0.4966\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4819 - acc: 0.4530 - val_loss: 1.3786 - val_acc: 0.5040\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4780 - acc: 0.4543 - val_loss: 1.3618 - val_acc: 0.5206\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4615 - acc: 0.4596 - val_loss: 1.3523 - val_acc: 0.5235\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4564 - acc: 0.4546 - val_loss: 1.3299 - val_acc: 0.5263\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4423 - acc: 0.4731 - val_loss: 1.3171 - val_acc: 0.5292\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4330 - acc: 0.4658 - val_loss: 1.3093 - val_acc: 0.5367\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4154 - acc: 0.4778 - val_loss: 1.3021 - val_acc: 0.5418\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4103 - acc: 0.4757 - val_loss: 1.2850 - val_acc: 0.5590\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 7s 1ms/step\n",
      ">> * New best model, loss: 0.0\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 25s 4ms/step - loss: 2.2167 - acc: 0.1857 - val_loss: 1.9232 - val_acc: 0.2938\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9778 - acc: 0.2638 - val_loss: 1.8976 - val_acc: 0.2875\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8853 - acc: 0.2846 - val_loss: 1.9145 - val_acc: 0.2400\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8312 - acc: 0.3006 - val_loss: 1.8801 - val_acc: 0.3081\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7865 - acc: 0.3142 - val_loss: 1.8812 - val_acc: 0.3110\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7445 - acc: 0.3367 - val_loss: 1.8893 - val_acc: 0.2978\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7219 - acc: 0.3414 - val_loss: 1.8337 - val_acc: 0.3253\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6996 - acc: 0.3560 - val_loss: 1.7397 - val_acc: 0.3643\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6703 - acc: 0.3630 - val_loss: 1.6869 - val_acc: 0.3608\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6418 - acc: 0.3861 - val_loss: 1.6643 - val_acc: 0.3603\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6378 - acc: 0.3863 - val_loss: 1.6471 - val_acc: 0.3717\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6187 - acc: 0.3989 - val_loss: 1.6459 - val_acc: 0.3860\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 24s 3ms/step - loss: 1.5983 - acc: 0.4092 - val_loss: 1.6322 - val_acc: 0.3929\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5932 - acc: 0.4081 - val_loss: 1.6195 - val_acc: 0.4021\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5739 - acc: 0.4160 - val_loss: 1.5774 - val_acc: 0.4261\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5626 - acc: 0.4284 - val_loss: 1.5475 - val_acc: 0.4479\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5520 - acc: 0.4266 - val_loss: 1.5036 - val_acc: 0.4759\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5307 - acc: 0.4377 - val_loss: 1.4793 - val_acc: 0.4851\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 24s 3ms/step - loss: 1.5204 - acc: 0.4488 - val_loss: 1.4639 - val_acc: 0.4840\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5245 - acc: 0.4395 - val_loss: 1.4369 - val_acc: 0.5063\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 5\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 26s 4ms/step - loss: 2.2182 - acc: 0.1980 - val_loss: 1.9490 - val_acc: 0.3282\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9638 - acc: 0.3065 - val_loss: 1.8004 - val_acc: 0.3918\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8681 - acc: 0.3437 - val_loss: 1.7379 - val_acc: 0.4152\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8086 - acc: 0.3577 - val_loss: 1.6946 - val_acc: 0.4221\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7681 - acc: 0.3693 - val_loss: 1.6647 - val_acc: 0.4296\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7156 - acc: 0.3823 - val_loss: 1.6272 - val_acc: 0.4513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6746 - acc: 0.4025 - val_loss: 1.5942 - val_acc: 0.4565\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6552 - acc: 0.4120 - val_loss: 1.5697 - val_acc: 0.4622\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6199 - acc: 0.4200 - val_loss: 1.5452 - val_acc: 0.4679\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5991 - acc: 0.4304 - val_loss: 1.5296 - val_acc: 0.4754\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5790 - acc: 0.4366 - val_loss: 1.5209 - val_acc: 0.4840\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5572 - acc: 0.4347 - val_loss: 1.5082 - val_acc: 0.4777\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5461 - acc: 0.4393 - val_loss: 1.5015 - val_acc: 0.4685\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5185 - acc: 0.4605 - val_loss: 1.5064 - val_acc: 0.4656\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4986 - acc: 0.4656 - val_loss: 1.5098 - val_acc: 0.4553\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4820 - acc: 0.4652 - val_loss: 1.5122 - val_acc: 0.4479\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4709 - acc: 0.4678 - val_loss: 1.5268 - val_acc: 0.4353\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4439 - acc: 0.4784 - val_loss: 1.5249 - val_acc: 0.4462\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 24s 3ms/step - loss: 1.4386 - acc: 0.4752 - val_loss: 1.5618 - val_acc: 0.4307\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 24s 3ms/step - loss: 1.4289 - acc: 0.4754 - val_loss: 1.5515 - val_acc: 0.4330\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 26s 4ms/step - loss: 2.4286 - acc: 0.1774 - val_loss: 2.0607 - val_acc: 0.2497\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 2.0670 - acc: 0.2355 - val_loss: 1.9422 - val_acc: 0.2680\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9735 - acc: 0.2555 - val_loss: 1.9845 - val_acc: 0.2709\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8967 - acc: 0.2770 - val_loss: 2.0711 - val_acc: 0.2039\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.8451 - acc: 0.3013 - val_loss: 2.1345 - val_acc: 0.1919\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7931 - acc: 0.3221 - val_loss: 2.1097 - val_acc: 0.1907\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7599 - acc: 0.3322 - val_loss: 1.9821 - val_acc: 0.2428\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7275 - acc: 0.3371 - val_loss: 1.8208 - val_acc: 0.3276\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7083 - acc: 0.3506 - val_loss: 1.6960 - val_acc: 0.4101\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6814 - acc: 0.3617 - val_loss: 1.6255 - val_acc: 0.4479\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6715 - acc: 0.3652 - val_loss: 1.5998 - val_acc: 0.4341\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6559 - acc: 0.3729 - val_loss: 1.5972 - val_acc: 0.4393\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6368 - acc: 0.3798 - val_loss: 1.6018 - val_acc: 0.4422\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6295 - acc: 0.3931 - val_loss: 1.6177 - val_acc: 0.4370\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6184 - acc: 0.3938 - val_loss: 1.6264 - val_acc: 0.4318\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6041 - acc: 0.4055 - val_loss: 1.6341 - val_acc: 0.4318\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5848 - acc: 0.4113 - val_loss: 1.6095 - val_acc: 0.4456\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5817 - acc: 0.4120 - val_loss: 1.5859 - val_acc: 0.4553\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5618 - acc: 0.4270 - val_loss: 1.5551 - val_acc: 0.4691\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5593 - acc: 0.4220 - val_loss: 1.5408 - val_acc: 0.4851\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 7s 1ms/step\n",
      ">> * New best model, loss: 0.001\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 6\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 26s 4ms/step - loss: 2.2468 - acc: 0.1659 - val_loss: 1.9779 - val_acc: 0.2950\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9724 - acc: 0.2860 - val_loss: 1.8252 - val_acc: 0.3202\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8793 - acc: 0.3205 - val_loss: 1.7489 - val_acc: 0.3454\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8151 - acc: 0.3477 - val_loss: 1.6989 - val_acc: 0.3740\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7661 - acc: 0.3653 - val_loss: 1.6504 - val_acc: 0.3895\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7260 - acc: 0.3816 - val_loss: 1.6170 - val_acc: 0.4158\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6933 - acc: 0.3889 - val_loss: 1.5828 - val_acc: 0.4381\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6625 - acc: 0.3934 - val_loss: 1.5574 - val_acc: 0.4513\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6333 - acc: 0.4034 - val_loss: 1.5489 - val_acc: 0.4381\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5959 - acc: 0.4167 - val_loss: 1.5337 - val_acc: 0.4347\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5714 - acc: 0.4173 - val_loss: 1.4934 - val_acc: 0.4542\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5403 - acc: 0.4362 - val_loss: 1.4699 - val_acc: 0.4674\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5273 - acc: 0.4430 - val_loss: 1.4407 - val_acc: 0.4719\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5010 - acc: 0.4466 - val_loss: 1.4276 - val_acc: 0.4834\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4873 - acc: 0.4548 - val_loss: 1.4128 - val_acc: 0.4885\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4666 - acc: 0.4611 - val_loss: 1.3957 - val_acc: 0.4926\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4525 - acc: 0.4654 - val_loss: 1.3611 - val_acc: 0.5086\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4406 - acc: 0.4729 - val_loss: 1.3511 - val_acc: 0.5074\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4222 - acc: 0.4784 - val_loss: 1.3401 - val_acc: 0.5074\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4064 - acc: 0.4885 - val_loss: 1.3290 - val_acc: 0.5206\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 7s 1ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 26s 4ms/step - loss: 2.2143 - acc: 0.1837 - val_loss: 1.9625 - val_acc: 0.2623\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9611 - acc: 0.2472 - val_loss: 2.0017 - val_acc: 0.2383\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8917 - acc: 0.2693 - val_loss: 1.9916 - val_acc: 0.2801\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8151 - acc: 0.2947 - val_loss: 1.9356 - val_acc: 0.2932\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7789 - acc: 0.3066 - val_loss: 1.8510 - val_acc: 0.3179\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7521 - acc: 0.3304 - val_loss: 1.7510 - val_acc: 0.3797\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7214 - acc: 0.3344 - val_loss: 1.6816 - val_acc: 0.4261\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6934 - acc: 0.3536 - val_loss: 1.6444 - val_acc: 0.4611\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6787 - acc: 0.3643 - val_loss: 1.6276 - val_acc: 0.4439\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6561 - acc: 0.3822 - val_loss: 1.6362 - val_acc: 0.4210\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6356 - acc: 0.3971 - val_loss: 1.6809 - val_acc: 0.3969\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6182 - acc: 0.4011 - val_loss: 1.7363 - val_acc: 0.3585\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6053 - acc: 0.4014 - val_loss: 1.7294 - val_acc: 0.3557\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5846 - acc: 0.4181 - val_loss: 1.6421 - val_acc: 0.4078\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5705 - acc: 0.4204 - val_loss: 1.5492 - val_acc: 0.4381\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5685 - acc: 0.4247 - val_loss: 1.5199 - val_acc: 0.4536\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5547 - acc: 0.4332 - val_loss: 1.4964 - val_acc: 0.4548\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5424 - acc: 0.4439 - val_loss: 1.4772 - val_acc: 0.4605\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5407 - acc: 0.4376 - val_loss: 1.4748 - val_acc: 0.4651\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5248 - acc: 0.4508 - val_loss: 1.4719 - val_acc: 0.4765\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 7s 1ms/step\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 7\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 26s 4ms/step - loss: 2.1825 - acc: 0.2043 - val_loss: 1.8882 - val_acc: 0.3247\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9300 - acc: 0.3065 - val_loss: 1.7677 - val_acc: 0.3517\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8460 - acc: 0.3448 - val_loss: 1.7147 - val_acc: 0.3792\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.7903 - acc: 0.3652 - val_loss: 1.6907 - val_acc: 0.3803\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7387 - acc: 0.3895 - val_loss: 1.6788 - val_acc: 0.3683\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6865 - acc: 0.4027 - val_loss: 1.6739 - val_acc: 0.3711\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6698 - acc: 0.4031 - val_loss: 1.6452 - val_acc: 0.3900\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6373 - acc: 0.4180 - val_loss: 1.6262 - val_acc: 0.3969\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6113 - acc: 0.4190 - val_loss: 1.5705 - val_acc: 0.4210\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5837 - acc: 0.4299 - val_loss: 1.5062 - val_acc: 0.4473\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5564 - acc: 0.4390 - val_loss: 1.4902 - val_acc: 0.4576\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5389 - acc: 0.4483 - val_loss: 1.4804 - val_acc: 0.4616\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5185 - acc: 0.4533 - val_loss: 1.4476 - val_acc: 0.4794\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5091 - acc: 0.4523 - val_loss: 1.4243 - val_acc: 0.4857\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4757 - acc: 0.4593 - val_loss: 1.4071 - val_acc: 0.4931\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4749 - acc: 0.4605 - val_loss: 1.4104 - val_acc: 0.4994\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4561 - acc: 0.4619 - val_loss: 1.3873 - val_acc: 0.5006\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4474 - acc: 0.4698 - val_loss: 1.3753 - val_acc: 0.5092\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4286 - acc: 0.4774 - val_loss: 1.3675 - val_acc: 0.5132\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4217 - acc: 0.4669 - val_loss: 1.3536 - val_acc: 0.5183\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 26s 4ms/step - loss: 2.3011 - acc: 0.1592 - val_loss: 2.0044 - val_acc: 0.2188\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 2.0398 - acc: 0.2345 - val_loss: 1.9034 - val_acc: 0.3104\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9511 - acc: 0.2730 - val_loss: 1.8470 - val_acc: 0.3018\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8981 - acc: 0.2890 - val_loss: 1.7916 - val_acc: 0.3058\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.8460 - acc: 0.2966 - val_loss: 1.7445 - val_acc: 0.3339\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7910 - acc: 0.3216 - val_loss: 1.7135 - val_acc: 0.3763\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7506 - acc: 0.3361 - val_loss: 1.7021 - val_acc: 0.4049\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7098 - acc: 0.3580 - val_loss: 1.7092 - val_acc: 0.3940\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6893 - acc: 0.3652 - val_loss: 1.7167 - val_acc: 0.3603\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6630 - acc: 0.3765 - val_loss: 1.7361 - val_acc: 0.3477\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6429 - acc: 0.3956 - val_loss: 1.7338 - val_acc: 0.3471\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6299 - acc: 0.3906 - val_loss: 1.7047 - val_acc: 0.3580\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6172 - acc: 0.3924 - val_loss: 1.6844 - val_acc: 0.3603\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5996 - acc: 0.4072 - val_loss: 1.6418 - val_acc: 0.3940\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5808 - acc: 0.4091 - val_loss: 1.5979 - val_acc: 0.4170\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5726 - acc: 0.4254 - val_loss: 1.5402 - val_acc: 0.4507\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5620 - acc: 0.4238 - val_loss: 1.5002 - val_acc: 0.4731\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5502 - acc: 0.4254 - val_loss: 1.4891 - val_acc: 0.4748\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5382 - acc: 0.4344 - val_loss: 1.4602 - val_acc: 0.4811\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5363 - acc: 0.4343 - val_loss: 1.4419 - val_acc: 0.5017\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 8\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 27s 4ms/step - loss: 2.1257 - acc: 0.2309 - val_loss: 1.8392 - val_acc: 0.3820\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.9129 - acc: 0.3192 - val_loss: 1.7011 - val_acc: 0.4107\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.8132 - acc: 0.3570 - val_loss: 1.6371 - val_acc: 0.4416\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7562 - acc: 0.3810 - val_loss: 1.5798 - val_acc: 0.4668\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6939 - acc: 0.4071 - val_loss: 1.5457 - val_acc: 0.4674\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6665 - acc: 0.4102 - val_loss: 1.5307 - val_acc: 0.4588\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6246 - acc: 0.4154 - val_loss: 1.5167 - val_acc: 0.4536\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5881 - acc: 0.4289 - val_loss: 1.5093 - val_acc: 0.4525\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5635 - acc: 0.4297 - val_loss: 1.5099 - val_acc: 0.4502\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5398 - acc: 0.4380 - val_loss: 1.4908 - val_acc: 0.4593\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5088 - acc: 0.4475 - val_loss: 1.4496 - val_acc: 0.4696\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4988 - acc: 0.4490 - val_loss: 1.4114 - val_acc: 0.4737\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4753 - acc: 0.4603 - val_loss: 1.3777 - val_acc: 0.4948\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4586 - acc: 0.4722 - val_loss: 1.3524 - val_acc: 0.5023\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4526 - acc: 0.4658 - val_loss: 1.3181 - val_acc: 0.5218\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4485 - acc: 0.4655 - val_loss: 1.2857 - val_acc: 0.5309\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4292 - acc: 0.4768 - val_loss: 1.2904 - val_acc: 0.5263\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4170 - acc: 0.4787 - val_loss: 1.3024 - val_acc: 0.5137\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4087 - acc: 0.4807 - val_loss: 1.2882 - val_acc: 0.5183\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.3943 - acc: 0.4930 - val_loss: 1.2656 - val_acc: 0.5326\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 27s 4ms/step - loss: 2.2798 - acc: 0.1849 - val_loss: 1.9637 - val_acc: 0.2239\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9754 - acc: 0.2714 - val_loss: 2.0305 - val_acc: 0.1982\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8826 - acc: 0.2952 - val_loss: 2.0042 - val_acc: 0.2434\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.8312 - acc: 0.3068 - val_loss: 1.8995 - val_acc: 0.2973\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7888 - acc: 0.3245 - val_loss: 1.8240 - val_acc: 0.3121\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7547 - acc: 0.3378 - val_loss: 1.7490 - val_acc: 0.3351\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 24s 3ms/step - loss: 1.7257 - acc: 0.3506 - val_loss: 1.7156 - val_acc: 0.3608\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6969 - acc: 0.3571 - val_loss: 1.6962 - val_acc: 0.3803\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6781 - acc: 0.3680 - val_loss: 1.7000 - val_acc: 0.3803\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6598 - acc: 0.3702 - val_loss: 1.7313 - val_acc: 0.3585\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6480 - acc: 0.3789 - val_loss: 1.7582 - val_acc: 0.3494\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6252 - acc: 0.3908 - val_loss: 1.7684 - val_acc: 0.3540\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6155 - acc: 0.3979 - val_loss: 1.7456 - val_acc: 0.3694\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6090 - acc: 0.3995 - val_loss: 1.6225 - val_acc: 0.4118\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5971 - acc: 0.4050 - val_loss: 1.5536 - val_acc: 0.4267\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5911 - acc: 0.4174 - val_loss: 1.5594 - val_acc: 0.4215\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5693 - acc: 0.4213 - val_loss: 1.5464 - val_acc: 0.4215\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5525 - acc: 0.4294 - val_loss: 1.5100 - val_acc: 0.4267\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5524 - acc: 0.4301 - val_loss: 1.4899 - val_acc: 0.4359\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5557 - acc: 0.4310 - val_loss: 1.5056 - val_acc: 0.4250\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 9s 1ms/step\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 9\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 27s 4ms/step - loss: 2.1930 - acc: 0.2100 - val_loss: 1.8716 - val_acc: 0.3379\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.9308 - acc: 0.3085 - val_loss: 1.7742 - val_acc: 0.3786\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8540 - acc: 0.3252 - val_loss: 1.7282 - val_acc: 0.3717\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.7906 - acc: 0.3504 - val_loss: 1.6982 - val_acc: 0.3849\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.7459 - acc: 0.3696 - val_loss: 1.6709 - val_acc: 0.4078\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7055 - acc: 0.3861 - val_loss: 1.6320 - val_acc: 0.4227\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6793 - acc: 0.3959 - val_loss: 1.5990 - val_acc: 0.4313\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6617 - acc: 0.3945 - val_loss: 1.5861 - val_acc: 0.4244\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6182 - acc: 0.4178 - val_loss: 1.5799 - val_acc: 0.4227\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6164 - acc: 0.4092 - val_loss: 1.5530 - val_acc: 0.4296\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5743 - acc: 0.4241 - val_loss: 1.5398 - val_acc: 0.4399\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5609 - acc: 0.4247 - val_loss: 1.5216 - val_acc: 0.4519\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5367 - acc: 0.4419 - val_loss: 1.5185 - val_acc: 0.4502\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5247 - acc: 0.4440 - val_loss: 1.5230 - val_acc: 0.4507\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4990 - acc: 0.4509 - val_loss: 1.5500 - val_acc: 0.4439\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4800 - acc: 0.4520 - val_loss: 1.5081 - val_acc: 0.4565\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4697 - acc: 0.4593 - val_loss: 1.4805 - val_acc: 0.4599\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4567 - acc: 0.4658 - val_loss: 1.4632 - val_acc: 0.4754\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4427 - acc: 0.4684 - val_loss: 1.4765 - val_acc: 0.4748\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4263 - acc: 0.4711 - val_loss: 1.4369 - val_acc: 0.4817\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 27s 4ms/step - loss: 2.4101 - acc: 0.1789 - val_loss: 1.9942 - val_acc: 0.2279\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 2.0192 - acc: 0.2478 - val_loss: 1.8853 - val_acc: 0.2772\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9422 - acc: 0.2588 - val_loss: 1.8561 - val_acc: 0.3064\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8793 - acc: 0.2867 - val_loss: 1.8487 - val_acc: 0.3305\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8321 - acc: 0.2989 - val_loss: 1.8710 - val_acc: 0.3070\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7915 - acc: 0.3115 - val_loss: 1.8828 - val_acc: 0.3127\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7579 - acc: 0.3261 - val_loss: 1.8785 - val_acc: 0.2881\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7235 - acc: 0.3455 - val_loss: 1.8495 - val_acc: 0.2841\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7153 - acc: 0.3504 - val_loss: 1.7688 - val_acc: 0.3265\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6878 - acc: 0.3573 - val_loss: 1.7231 - val_acc: 0.3511\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6650 - acc: 0.3690 - val_loss: 1.7033 - val_acc: 0.3625\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6551 - acc: 0.3808 - val_loss: 1.6905 - val_acc: 0.3809\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6420 - acc: 0.3849 - val_loss: 1.6736 - val_acc: 0.3969\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6205 - acc: 0.3932 - val_loss: 1.6608 - val_acc: 0.4003\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 24s 3ms/step - loss: 1.6079 - acc: 0.3946 - val_loss: 1.6498 - val_acc: 0.3952\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5951 - acc: 0.4085 - val_loss: 1.6345 - val_acc: 0.3981\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5735 - acc: 0.4210 - val_loss: 1.5975 - val_acc: 0.4273\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5821 - acc: 0.4211 - val_loss: 1.5472 - val_acc: 0.4593\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5714 - acc: 0.4253 - val_loss: 1.5181 - val_acc: 0.4588\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5501 - acc: 0.4332 - val_loss: 1.5064 - val_acc: 0.4622\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 10\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 27s 4ms/step - loss: 2.2415 - acc: 0.1834 - val_loss: 1.9625 - val_acc: 0.3144\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9809 - acc: 0.2972 - val_loss: 1.8091 - val_acc: 0.3940\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8969 - acc: 0.3390 - val_loss: 1.7468 - val_acc: 0.3958\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8332 - acc: 0.3566 - val_loss: 1.6983 - val_acc: 0.4347\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7873 - acc: 0.3664 - val_loss: 1.6639 - val_acc: 0.4410\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7407 - acc: 0.3914 - val_loss: 1.6356 - val_acc: 0.4370\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7148 - acc: 0.3956 - val_loss: 1.6054 - val_acc: 0.4370\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6792 - acc: 0.3995 - val_loss: 1.5808 - val_acc: 0.4364\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6517 - acc: 0.4077 - val_loss: 1.5580 - val_acc: 0.4353\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6279 - acc: 0.4247 - val_loss: 1.5362 - val_acc: 0.4439\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6074 - acc: 0.4155 - val_loss: 1.5182 - val_acc: 0.4485\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5819 - acc: 0.4357 - val_loss: 1.4997 - val_acc: 0.4542\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5674 - acc: 0.4330 - val_loss: 1.4859 - val_acc: 0.4651\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5481 - acc: 0.4495 - val_loss: 1.4696 - val_acc: 0.4719\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5344 - acc: 0.4505 - val_loss: 1.4591 - val_acc: 0.4800\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5166 - acc: 0.4535 - val_loss: 1.4605 - val_acc: 0.4748\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5034 - acc: 0.4542 - val_loss: 1.4754 - val_acc: 0.4582\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 24s 3ms/step - loss: 1.4856 - acc: 0.4536 - val_loss: 1.4761 - val_acc: 0.4651\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4659 - acc: 0.4658 - val_loss: 1.4917 - val_acc: 0.4588\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.4522 - acc: 0.4681 - val_loss: 1.4707 - val_acc: 0.4662\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 28s 4ms/step - loss: 2.3226 - acc: 0.1844 - val_loss: 1.9540 - val_acc: 0.3007\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9944 - acc: 0.2505 - val_loss: 1.8436 - val_acc: 0.2738\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.9068 - acc: 0.2761 - val_loss: 1.8312 - val_acc: 0.3007\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 22s 3ms/step - loss: 1.8529 - acc: 0.2959 - val_loss: 1.8172 - val_acc: 0.3162\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.8048 - acc: 0.3106 - val_loss: 1.7986 - val_acc: 0.3207\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7556 - acc: 0.3299 - val_loss: 1.7790 - val_acc: 0.3345\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7317 - acc: 0.3328 - val_loss: 1.7451 - val_acc: 0.3545\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.7039 - acc: 0.3564 - val_loss: 1.7161 - val_acc: 0.3855\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6856 - acc: 0.3591 - val_loss: 1.6959 - val_acc: 0.3975\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 24s 3ms/step - loss: 1.6718 - acc: 0.3697 - val_loss: 1.6717 - val_acc: 0.4026\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6489 - acc: 0.3792 - val_loss: 1.6359 - val_acc: 0.4198\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6365 - acc: 0.3926 - val_loss: 1.5999 - val_acc: 0.4485\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6114 - acc: 0.4011 - val_loss: 1.5899 - val_acc: 0.4467\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.6015 - acc: 0.3998 - val_loss: 1.5953 - val_acc: 0.4381\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5884 - acc: 0.4124 - val_loss: 1.5687 - val_acc: 0.4559\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5772 - acc: 0.4231 - val_loss: 1.5368 - val_acc: 0.4702\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5844 - acc: 0.4127 - val_loss: 1.5139 - val_acc: 0.4719\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5591 - acc: 0.4276 - val_loss: 1.5110 - val_acc: 0.4639\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5634 - acc: 0.4296 - val_loss: 1.5000 - val_acc: 0.4651\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 23s 3ms/step - loss: 1.5515 - acc: 0.4313 - val_loss: 1.5040 - val_acc: 0.4662\n",
      "1746/1746 [==============================] - 2s 1ms/step\n",
      "6986/6986 [==============================] - 8s 1ms/step\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n"
     ]
    }
   ],
   "source": [
    "# Cyclic training settings\n",
    "rounds = 10\n",
    "max_loss_diff = 5.5\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "# Register round results\n",
    "orig_train_losses = []\n",
    "orig_test_losses = []\n",
    "orig_train_accuracies = []\n",
    "orig_test_accuracies = []\n",
    "orig_loss_diffs = []\n",
    "orig_acc_diffs = []\n",
    "norm_train_losses = []\n",
    "norm_test_losses = []\n",
    "norm_train_accuracies = []\n",
    "norm_test_accuracies = []\n",
    "norm_loss_diffs = []\n",
    "norm_acc_diffs = []\n",
    "winners = []\n",
    "\n",
    "# Best models\n",
    "best_orig_overall = 100\n",
    "best_orig_model = None\n",
    "best_orig_history = None\n",
    "best_norm_overall = 100\n",
    "best_norm_model = None\n",
    "best_norm_history = None\n",
    "\n",
    "# Input / Output shapes\n",
    "num_rows = 40\n",
    "num_columns = 174\n",
    "num_channels = 1\n",
    "num_labels = 10\n",
    "input_shape = (num_rows, num_columns, num_channels)\n",
    "\n",
    "# Iterate rounds\n",
    "for idx in range(rounds):\n",
    "\n",
    "    ### \n",
    "    ###   Round setup\n",
    "    ###\n",
    "\n",
    "    # Same randomized set for both data sources\n",
    "    rand_indexes = randomize(num_samples)\n",
    "\n",
    "    # Prepare both matrices: split() returns [X_train, y_train, X_test, y_test]\n",
    "    data_orig = split(X_orig, y_orig, rand_indexes)\n",
    "    data_norm = split(X_norm, y_norm, rand_indexes)\n",
    "\n",
    "\n",
    "    print(\">>> Round #\", idx+1)\n",
    "    \n",
    "\n",
    "    ### \n",
    "    ###   Standard data\n",
    "    ###\n",
    "\n",
    "    print(\">> Training with STANDARD data...\")\n",
    "    \n",
    "    # Create momdel (original dataset)\n",
    "    orig_model = create_model(input_shape, num_labels)\n",
    "\n",
    "    # Train model and fetch results\n",
    "    orig_results = train_model(orig_model,\n",
    "                               data_orig[0], \n",
    "                               data_orig[1], \n",
    "                               data_orig[2], \n",
    "                               data_orig[3], \n",
    "                               num_epochs, \n",
    "                               batch_size)\n",
    "\n",
    "    # Register round results\n",
    "    orig_train_losses.append(orig_results['train_loss'])\n",
    "    orig_test_losses.append(orig_results['test_loss'])\n",
    "    orig_train_accuracies.append(orig_results['train_acc'])\n",
    "    orig_test_accuracies.append(orig_results['test_acc'])\n",
    "    orig_loss_diffs.append(orig_results['loss_diff'])\n",
    "    orig_acc_diffs.append(orig_results['acc_diff'])\n",
    "\n",
    "    # Check best original model\n",
    "    if (orig_results['loss_diff'] < best_orig_overall):\n",
    "        best_orig_overall = orig_results['loss_diff']\n",
    "        best_orig_model = orig_model\n",
    "        best_orig_history = orig_results['history']\n",
    "        print(\">> * New best model, loss: {}\".format(best_orig_overall))\n",
    "\n",
    "\n",
    "\n",
    "    ### \n",
    "    ###   Normalized data\n",
    "    ###\n",
    "\n",
    "    print(\">> Training with NORMALIZED data...\")\n",
    "    \n",
    "    # Create momdel (original dataset)\n",
    "    norm_model = create_model(input_shape, num_labels)\n",
    "\n",
    "    # Train model and fetch results\n",
    "    norm_results = train_model(norm_model,\n",
    "                               data_norm[0], \n",
    "                               data_norm[1], \n",
    "                               data_norm[2], \n",
    "                               data_norm[3], \n",
    "                               num_epochs, \n",
    "                               batch_size)\n",
    "\n",
    "    # Register round results\n",
    "    norm_train_losses.append(norm_results['train_loss'])\n",
    "    norm_test_losses.append(norm_results['test_loss'])\n",
    "    norm_train_accuracies.append(norm_results['train_acc'])\n",
    "    norm_test_accuracies.append(norm_results['test_acc'])\n",
    "    norm_loss_diffs.append(norm_results['loss_diff'])\n",
    "    norm_acc_diffs.append(norm_results['acc_diff'])\n",
    "\n",
    "    # Check best original model\n",
    "    if (norm_results['loss_diff'] < best_norm_overall):\n",
    "        best_norm_overall = norm_results['loss_diff']\n",
    "        best_norm_model = norm_model\n",
    "        best_norm_history = norm_results['history']\n",
    "        print(\">> * New best model, loss: {}\".format(best_norm_overall))\n",
    "\n",
    "    # Register round winner\n",
    "    if ((norm_results['loss_diff'] < max_loss_diff) & (norm_results['test_loss'] < orig_results['test_loss'])):\n",
    "        round_winner = 'normalized'\n",
    "    elif (orig_results['loss_diff'] < max_loss_diff):\n",
    "        round_winner = 'standard'\n",
    "    else:\n",
    "        round_winner = 'none'\n",
    "        \n",
    "    print(\">> Round winner: {}\".format(round_winner))\n",
    "    \n",
    "    # Register winner\n",
    "    winners.append(round_winner)\n",
    "    \n",
    "    print(\">>> Finished training tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_train_loss</th>\n",
       "      <th>norm_train_loss</th>\n",
       "      <th>orig_test_loss</th>\n",
       "      <th>norm_test_loss</th>\n",
       "      <th>orig_train_accuracy</th>\n",
       "      <th>norm_train_accuracy</th>\n",
       "      <th>orig_test_accuracy</th>\n",
       "      <th>norm_test_accuracy</th>\n",
       "      <th>orig_loss_diff</th>\n",
       "      <th>norm_loss_diff</th>\n",
       "      <th>orig_acc_diff</th>\n",
       "      <th>norm_acc_diff</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.533625</td>\n",
       "      <td>1.488913</td>\n",
       "      <td>1.513861</td>\n",
       "      <td>1.479488</td>\n",
       "      <td>44.730813</td>\n",
       "      <td>48.109966</td>\n",
       "      <td>46.263956</td>\n",
       "      <td>48.912110</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.533</td>\n",
       "      <td>0.802</td>\n",
       "      <td>normalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.416530</td>\n",
       "      <td>1.472096</td>\n",
       "      <td>1.391388</td>\n",
       "      <td>1.479056</td>\n",
       "      <td>48.339061</td>\n",
       "      <td>49.427262</td>\n",
       "      <td>50.658460</td>\n",
       "      <td>50.128829</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.007</td>\n",
       "      <td>2.319</td>\n",
       "      <td>0.702</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.352776</td>\n",
       "      <td>1.484936</td>\n",
       "      <td>1.324364</td>\n",
       "      <td>1.486450</td>\n",
       "      <td>50.801833</td>\n",
       "      <td>49.713631</td>\n",
       "      <td>51.903808</td>\n",
       "      <td>48.811910</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.102</td>\n",
       "      <td>0.902</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.285035</td>\n",
       "      <td>1.436858</td>\n",
       "      <td>1.285409</td>\n",
       "      <td>1.443584</td>\n",
       "      <td>55.899198</td>\n",
       "      <td>50.630011</td>\n",
       "      <td>53.707415</td>\n",
       "      <td>49.040939</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>2.192</td>\n",
       "      <td>1.589</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.551502</td>\n",
       "      <td>1.540803</td>\n",
       "      <td>1.515081</td>\n",
       "      <td>1.539430</td>\n",
       "      <td>43.298969</td>\n",
       "      <td>48.510882</td>\n",
       "      <td>45.991984</td>\n",
       "      <td>47.824220</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.693</td>\n",
       "      <td>0.687</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.329039</td>\n",
       "      <td>1.471902</td>\n",
       "      <td>1.331590</td>\n",
       "      <td>1.489023</td>\n",
       "      <td>52.061856</td>\n",
       "      <td>47.651775</td>\n",
       "      <td>52.590896</td>\n",
       "      <td>45.419410</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.529</td>\n",
       "      <td>2.232</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.353611</td>\n",
       "      <td>1.441864</td>\n",
       "      <td>1.338708</td>\n",
       "      <td>1.463891</td>\n",
       "      <td>51.832761</td>\n",
       "      <td>50.171821</td>\n",
       "      <td>51.646149</td>\n",
       "      <td>47.738334</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.187</td>\n",
       "      <td>2.433</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.265618</td>\n",
       "      <td>1.505600</td>\n",
       "      <td>1.304079</td>\n",
       "      <td>1.532081</td>\n",
       "      <td>53.264605</td>\n",
       "      <td>42.497136</td>\n",
       "      <td>52.247352</td>\n",
       "      <td>43.143430</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.646</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.436856</td>\n",
       "      <td>1.506378</td>\n",
       "      <td>1.417595</td>\n",
       "      <td>1.489132</td>\n",
       "      <td>48.167239</td>\n",
       "      <td>46.219931</td>\n",
       "      <td>49.141139</td>\n",
       "      <td>47.265960</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.046</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.470692</td>\n",
       "      <td>1.503957</td>\n",
       "      <td>1.451872</td>\n",
       "      <td>1.487771</td>\n",
       "      <td>46.620848</td>\n",
       "      <td>46.620848</td>\n",
       "      <td>47.380475</td>\n",
       "      <td>47.366161</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.745</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   orig_train_loss  norm_train_loss  orig_test_loss  norm_test_loss  \\\n",
       "0         1.533625         1.488913        1.513861        1.479488   \n",
       "1         1.416530         1.472096        1.391388        1.479056   \n",
       "2         1.352776         1.484936        1.324364        1.486450   \n",
       "3         1.285035         1.436858        1.285409        1.443584   \n",
       "4         1.551502         1.540803        1.515081        1.539430   \n",
       "5         1.329039         1.471902        1.331590        1.489023   \n",
       "6         1.353611         1.441864        1.338708        1.463891   \n",
       "7         1.265618         1.505600        1.304079        1.532081   \n",
       "8         1.436856         1.506378        1.417595        1.489132   \n",
       "9         1.470692         1.503957        1.451872        1.487771   \n",
       "\n",
       "   orig_train_accuracy  norm_train_accuracy  orig_test_accuracy  \\\n",
       "0            44.730813            48.109966           46.263956   \n",
       "1            48.339061            49.427262           50.658460   \n",
       "2            50.801833            49.713631           51.903808   \n",
       "3            55.899198            50.630011           53.707415   \n",
       "4            43.298969            48.510882           45.991984   \n",
       "5            52.061856            47.651775           52.590896   \n",
       "6            51.832761            50.171821           51.646149   \n",
       "7            53.264605            42.497136           52.247352   \n",
       "8            48.167239            46.219931           49.141139   \n",
       "9            46.620848            46.620848           47.380475   \n",
       "\n",
       "   norm_test_accuracy  orig_loss_diff  norm_loss_diff  orig_acc_diff  \\\n",
       "0           48.912110           0.020           0.009          1.533   \n",
       "1           50.128829           0.025           0.007          2.319   \n",
       "2           48.811910           0.028           0.002          1.102   \n",
       "3           49.040939           0.000           0.007          2.192   \n",
       "4           47.824220           0.036           0.001          2.693   \n",
       "5           45.419410           0.003           0.017          0.529   \n",
       "6           47.738334           0.015           0.022          0.187   \n",
       "7           43.143430           0.038           0.026          1.017   \n",
       "8           47.265960           0.019           0.017          0.974   \n",
       "9           47.366161           0.019           0.016          0.760   \n",
       "\n",
       "   norm_acc_diff      winner  \n",
       "0          0.802  normalized  \n",
       "1          0.702    standard  \n",
       "2          0.902    standard  \n",
       "3          1.589    standard  \n",
       "4          0.687    standard  \n",
       "5          2.232    standard  \n",
       "6          2.433    standard  \n",
       "7          0.646    standard  \n",
       "8          1.046    standard  \n",
       "9          0.745    standard  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save registry\n",
    "registry = pd.DataFrame({\n",
    "    'orig_train_loss': orig_train_losses,\n",
    "    'norm_train_loss': norm_train_losses,\n",
    "    'orig_test_loss': orig_test_losses,\n",
    "    'norm_test_loss': norm_test_losses,\n",
    "    'orig_train_accuracy': orig_train_accuracies,\n",
    "    'norm_train_accuracy': norm_train_accuracies,\n",
    "    'orig_test_accuracy': orig_test_accuracies,\n",
    "    'norm_test_accuracy': norm_test_accuracies,\n",
    "    'orig_loss_diff': orig_loss_diffs,\n",
    "    'norm_loss_diff': norm_loss_diffs,\n",
    "    'orig_acc_diff': orig_acc_diffs,\n",
    "    'norm_acc_diff': norm_acc_diffs,\n",
    "    'winner': winners\n",
    "})\n",
    "\n",
    "registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry.to_csv('./data/appendis-4.csv',encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/best_orig_history_2', 'wb') as file_pi:\n",
    "    pickle.dump(best_orig_history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train/Test loss difference mean\n",
      "Standard: 0.04920000000000001\n",
      "Normalized: 0.048400000000000006\n",
      "\n",
      ">> Train/Test accuracy difference mean\n",
      "Standard: 2.2024\n",
      "Normalized: 1.8474000000000004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\">> Train/Test loss difference mean\")\n",
    "print(\"Standard:\", registry['orig_loss_diff'].mean())\n",
    "print(\"Normalized:\", registry['norm_loss_diff'].mean())\n",
    "\n",
    "print(\"\\n>> Train/Test accuracy difference mean\")\n",
    "print(\"Standard:\", registry['orig_acc_diff'].mean())\n",
    "print(\"Normalized:\", registry['norm_acc_diff'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training history comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test loss comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard history for loss\n",
    "plt.plot(best_orig_history.history['loss'])\n",
    "plt.plot(best_orig_history.history['val_loss'])\n",
    "plt.title('Stadanrd Data - Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Normalized history for loss\n",
    "plt.plot(best_norm_history.history['loss'])\n",
    "plt.plot(best_norm_history.history['val_loss'])\n",
    "plt.title('Normalized Data - Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test accuracy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard history for accuracy\n",
    "plt.plot(best_orig_history.history['acc'])\n",
    "plt.plot(best_orig_history.history['val_acc'])\n",
    "plt.title('Standard Data - Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Standard history for accuracy\n",
    "plt.plot(best_norm_history.history['acc'])\n",
    "plt.plot(best_norm_history.history['val_acc'])\n",
    "plt.title('Normalized Data - Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "After 10 rounds of training with both datasets the results are:\n",
    "\n",
    "* Normalized: 4 wins\n",
    "* Standard: 6 wins\n",
    "\n",
    "The overall train/loss difference among all rounds is very similar, but watching the means we can tell that the models trained with normalized average amplitude data got a slight smaller value. Even though, the difference is very small. Also, the Train/Loss gap is more stable with the standard data, what should be better.\n",
    "\n",
    "Even though, in larger trainings (70+ epochs) I got more overfitted models using the normalized average amplitude data.\n",
    "\n",
    "In one of the many materials I have been following I found more clues that point to the use of NON-normalized audio for classification tasks like the one here:\n",
    "\n",
    "*As for normalization after db-scaling, that seems hit or miss depending on your data. From the paper above, the authors found nearly no difference using various normalization techniques for their data.*\n",
    "\n",
    "Source: https://stackoverflow.com/questions/55513652/which-spectrogram-best-represents-features-of-an-audio-file-for-cnn-based-model/56727927#56727927\n",
    "\n",
    "Related paper: [A Comparison of Audio Signal Preprocessing Methods for Deep Neural Networks on Music Tagging](https://arxiv.org/abs/1709.01922)\n",
    "\n",
    "I may continue experimenting with other type of normalization over the data that do not involve modifying the original audio dynamics. Or, in contrast, experiment augmenting data by random amplitude average alteration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (deep-learning-p36)",
   "language": "python",
   "name": "deep-learning-p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
