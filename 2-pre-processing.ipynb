{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifing audio data with convolutional neural networks\n",
    "\n",
    "<br/>\n",
    "by Eduardo Garcia Rajo @ 2019<br/>\n",
    "<br/>\n",
    "This notebook if part of my project \"Urban sounds classification with Covnolutional Neural Networks\" on my Githubat: https://github.com/GorillaBus/urban-audio-classifier.<br/>\n",
    "<br/>\n",
    "Licensed under the GNU LESSER GENERAL PUBLIC LICENSE Version 3, 29 June 2007<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "In this second notebook we are going evaluate how to convert our audio files to the same format to ensure we are working with consistent data. Then we'll discuss on feature extraction techniques and what is the best tool for the job.<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Pay attention to the very simple path variables configured in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "import sys\n",
    "import os\n",
    "import IPython as IP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your path to the dataset\n",
    "us8k_path = os.path.abspath('./UrbanSound8K')\n",
    "audio_path = os.path.join(us8k_path, 'audio')\n",
    "metadata_path = os.path.join(us8k_path, 'metadata/UrbanSound8K.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Generates MFCC coefficients with Librosa \n",
    "def get_mfcc(filename, mfcc_max_padding=0):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        # log-scaling \n",
    "        mfccs = librosa.core.power_to_db(mfccs)\n",
    "\n",
    "        # Should we require padding\n",
    "        pad_width = mfcc_max_padding - mfccs.shape[1]\n",
    "        if (mfcc_max_padding > 0 & mfccs.shape[1] < mfcc_max_padding):\n",
    "            pad_width = mfcc_max_padding - mfccs.shape[1]\n",
    "            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing wavefile: \", filename)\n",
    "        return None \n",
    "    return mfccs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the metadata from the generated CSV\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Examine dataframe\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarisation of digital audio\n",
    "Working with convolutions and audio data can be very different than working with image data. Actually, it can be more chanlenging.<br/>\n",
    "<br/>\n",
    "Ill try to give a brief explanation and later proposse reading material for if you like to investigate more in depth.<br/>\n",
    "<br/>\n",
    "As we already saw in the previous exploration we have a significant variance between the different audio properties (sample rate, bit depth, duration). There are some very important considerations we must take care of:<br/>\n",
    "<br/>\n",
    "#### Sample Rate\n",
    "Higher sample rates also mean wider frequency ranges. The higher the sample rate used to record a sound, the more frequencies being captured. More precisely, according to the [Nyquist Theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem), the highest frequency that can be captured in a recording is equal to the recording sample rate / 2, for example, while recording at 48.000 khz only frequencies up to 22.200 khz can be captured. This will also apply to resampling: while a 44.100 khz audio may contain frequencies up to 22.050 khz, when resampled to 22.050 khz the new file will contain half that range: up to 11.025 khz.<br/>\n",
    "<br/>\n",
    "At the same time, most of the recognizable changes in sound will occur in the lower frequencies and we can downsample to 22.050 khz, reducing the volume of data while still preserving the patterns our CNN will recognize.<br/>\n",
    "<br/>\n",
    "I strongly recommend to read this [well explained audio sampling material](http://support.ircam.fr/docs/AudioSculpt/3.0/co/Sampling.html)<br/>\n",
    "<br/>\n",
    "\n",
    "*Note that resampling audio may incurr in what is known as Aliassing. To overcome this common problem converters use \"Anti Aliassing Filters\". It is crucial that we use a conversor with well-implemented filters or the required audio features will be \"corrupted\" after the process, hence, wrong pattern recognition.*<br/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### Bit Depth\n",
    "Different bit depths mean higher resolution but also different scales. Converting bit depths is not problematic as converting sample rate (*see below).<br/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### Max Amplitude \n",
    "Each sound file will usually have a differente maximum point of amplitude. Ideally, we'd like to normalize this amplitude values to fit the same scale between all samples.<br/>\n",
    "<br/>\n",
    "\n",
    "#### Duration  \n",
    "A convolutional neural network requires to be feeded with spatially related data. Images are a good example: if you change the order of columns or rows, the position of pixels change and the information does not describe the originall image anymore. Similarly, different image sizes imply different spatial configurations.<br/>\n",
    "<br/>\n",
    "In sound we have that strict relation between frequencies and amplitudes over time, and having dataset of varying sound durations means varying input vector sizes to feed our CNN. To overcome this problem we'll use padding: for all the samples of length less than the maximum length in our data collection, we'll fill up space with zeros until we reach that maximum value. This is a commonly used technique and it does not alter the performance of the network.<br/>\n",
    "<br/>\n",
    "\n",
    "#### The good news\n",
    "Loading files with Librosa will automatically convert audio to 22.050 khz and 16 bit with tested quality audio conversion. Also, the data values will be normalized between 0 and 1. Using librosa, we just need to load files and we are almost ready to work with the data.\n",
    "\n",
    "The only disadvantage of Librosa, as a native Python implementation, is that it's much slower than ffmpeg to convert, and also much slower to extract audio features than a C++ library. But having tested with other audio libraries -like Essentia, a super fast C++ library with Python bindings- I can tell the audio features extracted with Librosa prooved to achieve higher scores, so I decided to sacrifice speed/time for qualilty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction: MFCC\n",
    "Extraction of Mel Frquency Cepstral Coefficients from the RAW audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is proved that you can feed a CNN with raw audio data and by doing 1D convolution the network is capable of finding patterns. Even thought, there are more efficient methods like feeding the CNN with the coeficients of the sound's spectograms. A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time.\n",
    "This showed better performance than using raw data.\n",
    "\n",
    "The typical spectrogram has one disadvantage: it uses a linear frequency scaling, so each frequency bin is spaced the equal number of Hertz apart. And here is how the MFCC comes in: the mel-frequency scale, on the other hand, is a quasi-logarithmic spacing roughly resembling the resolution of the human auditory system.\n",
    "\n",
    "For more detailed information [check this article](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/).<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Mel Spectogram of random file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /mnt/disks/disk-1/projects/urban-audio-classifier/UrbanSound8K/audio/fold6/155127-9-1-2.wav\n",
      "Category: street_music\n"
     ]
    }
   ],
   "source": [
    "# Get a random file\n",
    "row = metadata.sample(1)\n",
    "fold_num = str(row.iloc[0,5])\n",
    "file_name = str(row.iloc[0,0])\n",
    "category = str(row.iloc[0,7])\n",
    "file_path = audio_path + '/fold'+ fold_num +'/' + file_name\n",
    "\n",
    "# Display info\n",
    "print(\"File: \"+ file_path)\n",
    "print(\"Category: \"+ category)\n",
    "\n",
    "# Extract MFCCs from audio\n",
    "mfccs = get_mfcc(file_path)\n",
    "\n",
    "# Plot the MEL Spectogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfccs, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full dataset MFCC extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduugr/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 501/8732\n",
      "Status: 1001/8732\n",
      "Status: 1501/8732\n",
      "Status: 2001/8732\n",
      "Status: 2501/8732\n",
      "Status: 3001/8732\n",
      "Status: 3501/8732\n",
      "Status: 4001/8732\n",
      "Status: 4501/8732\n",
      "Status: 5001/8732\n",
      "Status: 5501/8732\n",
      "Status: 6001/8732\n",
      "Status: 6501/8732\n",
      "Status: 7001/8732\n",
      "Status: 7501/8732\n",
      "Status: 8001/8732\n",
      "Status: 8501/8732\n",
      "Finished: 8731/8732\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all audio files and extract MFCC\n",
    "features = []\n",
    "labels = []\n",
    "frames_max = 0\n",
    "counter = 0\n",
    "total_samples = len(metadata)\n",
    "\n",
    "for index, row in metadata.iterrows():\n",
    "    file_name = os.path.join(os.path.abspath(audio_path), 'fold' + str(row[\"fold\"]), str(row[\"slice_file_name\"]))\n",
    "    class_label = row[\"class\"]\n",
    "    mfccs = get_mfcc(file_name)    \n",
    "    num_frames = mfccs.shape[1]\n",
    "    \n",
    "    # Per-channel normalization\n",
    "    mean = np.mean(mfccs, axis=1, keepdims=True)\n",
    "    std = np.std(mfccs, axis=1, keepdims=True)\n",
    "    mfccs = (mfccs - mean) / std\n",
    "\n",
    "    \n",
    "    # Add row (feature / label)\n",
    "    features.append(mfccs)\n",
    "    labels.append(class_label)\n",
    "\n",
    "    # Update frames maximum\n",
    "    if (num_frames > frames_max):\n",
    "        frames_max = num_frames\n",
    "\n",
    "    # Notify update every N files\n",
    "    if (counter == 500):\n",
    "        print(\"Status: {}/{}\".format(index+1, total_samples))\n",
    "        counter = 0\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "print(\"Finished: {}/{}\".format(index, total_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add padding for a consistent shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = []\n",
    "\n",
    "# Add padding\n",
    "mfcc_max_padding = frames_max\n",
    "for i in range(len(features)):\n",
    "    size = len(features[i][0])\n",
    "    if (size < mfcc_max_padding):\n",
    "        pad_width = mfcc_max_padding - size\n",
    "        px = np.pad(features[i], \n",
    "                    pad_width=((0, 0), (0, pad_width)), \n",
    "                    mode='constant', \n",
    "                    constant_values=(0,))\n",
    "    \n",
    "    padded.append(px)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save X and y\n",
    "\n",
    "History:\n",
    "\n",
    "* v1 - MFCCs from original audio\n",
    "* v2 - MFCCs from audio with normalized average amplitude\n",
    "* v3 - MFCCs from original audio with log-scaling (essentially converting amplitude to a db scale)\n",
    "* v4 - Samme as v1 but padded with np.nan instead of zeros\n",
    "* v5 - MFCCs from original audio, with log-scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features (X) and labels (y) to Numpy arrays\n",
    "\n",
    "X = np.array(padded)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Optionally save the features to disk\n",
    "np.save(\"data/X-v5\", X)\n",
    "np.save(\"data/y-v5\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw features length: 8732\n",
      "Padded features length: 8732\n",
      "Feature labels length: 8732\n",
      "X: (8732, 40, 174), y: (8732,)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes\n",
    "print(\"Raw features length: {}\".format(len(features)))\n",
    "print(\"Padded features length: {}\".format(len(padded)))\n",
    "print(\"Feature labels length: {}\".format(len(features)))\n",
    "print(\"X: {}, y: {}\".format(X.shape, y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
