{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifing audio data with convolutional neural networks\n",
    "\n",
    "<br/>\n",
    "by Eduardo Garcia Rajo @ 2019<br/>\n",
    "<br/>\n",
    "This notebook if part of my project \"Urban sounds classification with Covnolutional Neural Networks\" on my Githubat: https://github.com/GorillaBus/urban-audio-classifier.<br/>\n",
    "<br/>\n",
    "Licensed under the GNU LESSER GENERAL PUBLIC LICENSE Version 3, 29 June 2007<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix II\n",
    "### Normalized audio amplitudes vs Original dataset amplitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same sound source can be captured with different amplitude levels depending on facts like recording settings, microphone characteristics, distance from the sensor to the source, etc.<br/>\n",
    "<br/>\n",
    "\n",
    "Normalizing the average peak amplitude in audio files seems to be a very common normalization process in speech recognition. It may also seem somewhat intuitive at first to normalize audio volumes; by normalizing the average amplitude of every sample in our dataset to the same value we are taking all those values to the same scale. But, at the same time, this amplitude difference between files in our dataset may also introduce a regularization effect that may help generalize better.<br/>\n",
    "<br/>\n",
    "\n",
    "We want to run an explicit test where we compare how a model perform when trained with normalized and varying average amplitude to resolve if we may normalize this feature or even if we may also want to use amplitude variation intentionally to augment data.<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import IPython\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime \n",
    "\n",
    "from keras import backend as keras_backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SpatialDropout2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define general variables/settings\n",
    "\n",
    "# Set your path to the dataset\n",
    "us8k_path = os.path.abspath('./UrbanSound8K')\n",
    "metadata_path = os.path.join(us8k_path, 'metadata/UrbanSound8K.csv')\n",
    "\n",
    "# Ensure \"channel last\" data format on Keras\n",
    "keras_backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata from the generated CSV\n",
    "metadata = pd.read_csv(metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "We are loading:\n",
    "* Data v1: the original version of the MFCC features extracted from the original audio\n",
    "* Data v2: MFCC features extracted from the normalized average peak amplitude dataset from appendix 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets: normalized (v2) and original (v1)\n",
    "num_samples = len(metadata)\n",
    "\n",
    "# Original\n",
    "X_orig = np.load(\"data/X-v1.npy\")[0:num_samples]\n",
    "y_orig = np.load(\"data/y-v1.npy\")[0:num_samples]\n",
    "\n",
    "# Features from the Normalized Avg Peak Amp dataset\n",
    "X_norm = np.load(\"data/X-v2.npy\")[0:num_samples]\n",
    "y_norm = np.load(\"data/y-v2.npy\")[0:num_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize indexes\n",
    "def randomize(total):\n",
    "    indexes = list(range(0, total))\n",
    "    \n",
    "    # Randomize indexes\n",
    "    random.shuffle(indexes)\n",
    "\n",
    "    # Divide the indexes into Train and Test\n",
    "    test_split_pct = 20\n",
    "    split_offset = math.floor(test_split_pct * total / 100)\n",
    "\n",
    "    # Split the metadata\n",
    "    test_split_idx = indexes[0:split_offset]\n",
    "    train_split_idx = indexes[split_offset:total]\n",
    "    test_meta = metadata.iloc[test_split_idx]\n",
    "    train_meta = metadata.iloc[train_split_idx]\n",
    "    \n",
    "    return [test_split_idx, train_split_idx, test_meta, train_meta]\n",
    "\n",
    "\n",
    "# Split data\n",
    "le = LabelEncoder()\n",
    "def split(X, y, splits):\n",
    "    # Split the features the with the same indexes\n",
    "    X_test = np.take(X, splits[0], axis=0)\n",
    "    y_test = np.take(y, splits[0], axis=0)\n",
    "    X_train = np.take(X, splits[1], axis=0)\n",
    "    y_train = np.take(y, splits[1], axis=0)\n",
    "    \n",
    "    # One-Hot\n",
    "    y_test_encoded = to_categorical(le.fit_transform(y_test))\n",
    "    y_train_encoded = to_categorical(le.fit_transform(y_train))\n",
    "\n",
    "    # Reshape to fit the network input (channel last!)\n",
    "    X_train = X_train.reshape(X_train.shape[0], num_rows, num_columns, num_channels)\n",
    "    X_test = X_test.reshape(X_test.shape[0], num_rows, num_columns, num_channels)\n",
    "    \n",
    "    return [X_train, y_train_encoded, X_test, y_test_encoded]\n",
    "\n",
    "\n",
    "# Create model\n",
    "def create_model(input_shape, num_labels):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Conv 1\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3,3), input_shape=input_shape, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(0.23))\n",
    "\n",
    "    # Conv 2\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(0.23))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Conv 3\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(0.23))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Conv 4\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SpatialDropout2D(0.23))\n",
    "\n",
    "    # Reduces each h√ów feature map to a single number by taking the average of all h,w values.\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    # Softmax output\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    adam = Adam(lr=1.5e-3, beta_1=0.99, beta_2=0.999)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy'], \n",
    "        optimizer=adam)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, num_epochs, batch_size):\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=num_epochs, \n",
    "                        validation_data=(X_test, y_test),\n",
    "                        verbose=1)\n",
    "\n",
    "    # Evaluate on train set\n",
    "    train_score = model.evaluate(X_test, y_test, verbose=1)\n",
    "    train_loss = train_score[0]\n",
    "    train_acc = 100 * train_score[1]\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_score = model.evaluate(X_train, y_train, verbose=1)\n",
    "    test_loss = test_score[0]\n",
    "    test_acc = 100 * test_score[1]\n",
    "\n",
    "    # Train / Test difference\n",
    "    loss_diff = round(abs(train_loss - test_loss), 3)\n",
    "    acc_diff = round(abs(train_acc - test_acc), 3)\n",
    "\n",
    "    return {\n",
    "        'history': history, \n",
    "        'train_loss': train_loss, \n",
    "        'test_loss': test_loss, \n",
    "        'train_acc': train_acc, \n",
    "        'test_acc': test_acc, \n",
    "        'loss_diff': loss_diff, \n",
    "        'acc_diff': acc_diff\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training comparison schedule\n",
    "\n",
    "* On each round I'm  shuffling the indexes of the data and creating two train/test splits using the same index orders with the two different data sources: original and normalized amplitudes.\n",
    "\n",
    "* Models are re-defined at the start of each round (they are never re-used).\n",
    "\n",
    "* A model is marked as 'winner' for a round in which it presents:\n",
    "    * a difference between Train/Test loss that is lower than 5.5% (a quite permeable value before considering overfitting)\n",
    "    * a lower test loss value than it's opponent\n",
    "\n",
    "* Finally we save from the best performing version of each data sources:\n",
    "    * the model itself\n",
    "    * its model history\n",
    "\n",
    "* Default settings: 10 rounds of 20 epochs per model\n",
    "\n",
    "* A per-round metrics registry is saved as a Pandas dataframe for further analysis\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 17:15:07.618076 140275136583488 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0722 17:15:07.788711 140275136583488 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Round # 1\n",
      ">> Training with STANDARD data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 17:15:07.859950 140275136583488 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0722 17:15:07.926643 140275136583488 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0722 17:15:07.927800 140275136583488 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0722 17:15:09.366298 140275136583488 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0722 17:15:09.460722 140275136583488 deprecation.py:506] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0722 17:15:09.576680 140275136583488 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0722 17:15:09.812932 140275136583488 deprecation_wrapper.py:119] From /home/eduugr/miniconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0722 17:15:10.035573 140275136583488 deprecation.py:323] From /home/eduugr/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 43s 6ms/step - loss: 1.8878 - acc: 0.3249 - val_loss: 1.4086 - val_acc: 0.5109\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.5469 - acc: 0.4453 - val_loss: 1.4271 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.4005 - acc: 0.4894 - val_loss: 1.2514 - val_acc: 0.5470\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.3182 - acc: 0.5215 - val_loss: 1.2431 - val_acc: 0.5515\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2631 - acc: 0.5465 - val_loss: 1.1245 - val_acc: 0.6134\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2215 - acc: 0.5591 - val_loss: 1.0501 - val_acc: 0.6352\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1701 - acc: 0.5830 - val_loss: 0.9939 - val_acc: 0.6523\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.1177 - acc: 0.6008 - val_loss: 0.9626 - val_acc: 0.6535\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.0865 - acc: 0.6167 - val_loss: 0.9491 - val_acc: 0.6667\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0483 - acc: 0.6267 - val_loss: 0.9405 - val_acc: 0.6919\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0380 - acc: 0.6386 - val_loss: 0.8616 - val_acc: 0.7085\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9872 - acc: 0.6529 - val_loss: 0.8044 - val_acc: 0.7245\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9699 - acc: 0.6629 - val_loss: 0.7898 - val_acc: 0.7405\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9496 - acc: 0.6658 - val_loss: 0.7812 - val_acc: 0.7331\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9185 - acc: 0.6791 - val_loss: 0.7849 - val_acc: 0.7360\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8852 - acc: 0.6974 - val_loss: 0.8083 - val_acc: 0.7274\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8784 - acc: 0.6944 - val_loss: 0.7509 - val_acc: 0.7577\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8416 - acc: 0.7129 - val_loss: 0.7044 - val_acc: 0.7715\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8253 - acc: 0.7154 - val_loss: 0.6708 - val_acc: 0.7944\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8116 - acc: 0.7272 - val_loss: 0.6707 - val_acc: 0.7950\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 11s 2ms/step\n",
      ">> * New best model, loss: 0.043\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 39s 6ms/step - loss: 1.9117 - acc: 0.3201 - val_loss: 1.6003 - val_acc: 0.4570\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.5887 - acc: 0.4382 - val_loss: 1.3567 - val_acc: 0.5029\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.4586 - acc: 0.4787 - val_loss: 1.2951 - val_acc: 0.5321\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.3580 - acc: 0.5137 - val_loss: 1.3069 - val_acc: 0.5281\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.2784 - acc: 0.5454 - val_loss: 1.2479 - val_acc: 0.5590\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.2397 - acc: 0.5571 - val_loss: 1.2464 - val_acc: 0.5510\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1870 - acc: 0.5760 - val_loss: 1.3262 - val_acc: 0.5538\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1439 - acc: 0.5949 - val_loss: 1.2226 - val_acc: 0.5916\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1021 - acc: 0.6111 - val_loss: 1.1902 - val_acc: 0.5945\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0704 - acc: 0.6198 - val_loss: 1.0844 - val_acc: 0.6340\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0247 - acc: 0.6479 - val_loss: 1.0532 - val_acc: 0.6312\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0042 - acc: 0.6459 - val_loss: 1.0463 - val_acc: 0.6489\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9801 - acc: 0.6514 - val_loss: 0.9540 - val_acc: 0.6816\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9461 - acc: 0.6682 - val_loss: 0.8677 - val_acc: 0.7085\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.9178 - acc: 0.6811 - val_loss: 0.8628 - val_acc: 0.7274\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.9144 - acc: 0.6766 - val_loss: 0.8355 - val_acc: 0.7268\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8798 - acc: 0.6935 - val_loss: 0.7862 - val_acc: 0.7457\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8597 - acc: 0.6993 - val_loss: 0.8091 - val_acc: 0.7342\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8474 - acc: 0.7081 - val_loss: 0.7887 - val_acc: 0.7446\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8095 - acc: 0.7259 - val_loss: 0.7414 - val_acc: 0.7652\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 12s 2ms/step\n",
      ">> * New best model, loss: 0.048\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 2\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 39s 6ms/step - loss: 1.8957 - acc: 0.3218 - val_loss: 1.5063 - val_acc: 0.4966\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.5591 - acc: 0.4528 - val_loss: 1.4279 - val_acc: 0.5235\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.4283 - acc: 0.4947 - val_loss: 1.2509 - val_acc: 0.5556\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.3225 - acc: 0.5253 - val_loss: 1.1619 - val_acc: 0.5956\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.2691 - acc: 0.5427 - val_loss: 1.0896 - val_acc: 0.6220\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.2113 - acc: 0.5673 - val_loss: 1.0496 - val_acc: 0.6340\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1387 - acc: 0.6002 - val_loss: 1.0216 - val_acc: 0.6277\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.1162 - acc: 0.6128 - val_loss: 0.9715 - val_acc: 0.6644\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0548 - acc: 0.6337 - val_loss: 0.9536 - val_acc: 0.6569\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0282 - acc: 0.6387 - val_loss: 0.9663 - val_acc: 0.6478\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9950 - acc: 0.6519 - val_loss: 1.0182 - val_acc: 0.6438\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9648 - acc: 0.6660 - val_loss: 0.9338 - val_acc: 0.6701\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9438 - acc: 0.6799 - val_loss: 0.9026 - val_acc: 0.6810\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9231 - acc: 0.6847 - val_loss: 0.8455 - val_acc: 0.6999\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8892 - acc: 0.6938 - val_loss: 0.7689 - val_acc: 0.7451\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8672 - acc: 0.7071 - val_loss: 0.7862 - val_acc: 0.7274\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8615 - acc: 0.7097 - val_loss: 0.7385 - val_acc: 0.7640\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8266 - acc: 0.7133 - val_loss: 0.7200 - val_acc: 0.7566\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8000 - acc: 0.7272 - val_loss: 0.7362 - val_acc: 0.7566\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.7979 - acc: 0.7259 - val_loss: 0.7095 - val_acc: 0.7789\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 12s 2ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 40s 6ms/step - loss: 1.8742 - acc: 0.3238 - val_loss: 1.6414 - val_acc: 0.4376\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.5776 - acc: 0.4367 - val_loss: 1.3344 - val_acc: 0.5309\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.4390 - acc: 0.4758 - val_loss: 1.3358 - val_acc: 0.5378\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.3362 - acc: 0.5220 - val_loss: 1.3811 - val_acc: 0.5263\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.2727 - acc: 0.5457 - val_loss: 1.3545 - val_acc: 0.5372\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.2106 - acc: 0.5706 - val_loss: 1.2313 - val_acc: 0.5630\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.1549 - acc: 0.5920 - val_loss: 1.1729 - val_acc: 0.5785\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.1053 - acc: 0.6078 - val_loss: 1.1316 - val_acc: 0.6060\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.0713 - acc: 0.6255 - val_loss: 1.0986 - val_acc: 0.6168\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.0369 - acc: 0.6391 - val_loss: 0.9921 - val_acc: 0.6569\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.9990 - acc: 0.6532 - val_loss: 0.9064 - val_acc: 0.6787\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.9686 - acc: 0.6622 - val_loss: 0.8888 - val_acc: 0.6896\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9397 - acc: 0.6755 - val_loss: 0.8949 - val_acc: 0.7005\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9013 - acc: 0.6848 - val_loss: 0.8462 - val_acc: 0.7079\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8676 - acc: 0.6967 - val_loss: 0.8935 - val_acc: 0.7039\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8467 - acc: 0.7084 - val_loss: 0.8493 - val_acc: 0.7102\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8385 - acc: 0.7164 - val_loss: 0.7319 - val_acc: 0.7520\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8102 - acc: 0.7272 - val_loss: 0.7367 - val_acc: 0.7451\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8065 - acc: 0.7234 - val_loss: 0.7507 - val_acc: 0.7400\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.7670 - acc: 0.7373 - val_loss: 0.7021 - val_acc: 0.7675\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 12s 2ms/step\n",
      ">> Round winner: normalized\n",
      ">>> Finished training tests\n",
      ">>> Round # 3\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 41s 6ms/step - loss: 1.9169 - acc: 0.3222 - val_loss: 1.6549 - val_acc: 0.4433\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.5548 - acc: 0.4425 - val_loss: 1.3020 - val_acc: 0.5395\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.4316 - acc: 0.4804 - val_loss: 1.2748 - val_acc: 0.5630\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3649 - acc: 0.5097 - val_loss: 1.2404 - val_acc: 0.5619\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2765 - acc: 0.5391 - val_loss: 1.2361 - val_acc: 0.5619\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.2146 - acc: 0.5581 - val_loss: 1.1429 - val_acc: 0.5991\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1609 - acc: 0.5845 - val_loss: 1.0925 - val_acc: 0.6157\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.1301 - acc: 0.5963 - val_loss: 1.0025 - val_acc: 0.6432\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.0871 - acc: 0.6184 - val_loss: 1.0067 - val_acc: 0.6672\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.0468 - acc: 0.6290 - val_loss: 0.9954 - val_acc: 0.6575\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.0282 - acc: 0.6429 - val_loss: 0.9389 - val_acc: 0.6827\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.9871 - acc: 0.6543 - val_loss: 0.9081 - val_acc: 0.6964\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.9570 - acc: 0.6663 - val_loss: 0.8544 - val_acc: 0.7113\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.9437 - acc: 0.6711 - val_loss: 0.8066 - val_acc: 0.7222\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9190 - acc: 0.6782 - val_loss: 0.7730 - val_acc: 0.7371\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.8975 - acc: 0.6842 - val_loss: 0.7501 - val_acc: 0.7503\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.8687 - acc: 0.6983 - val_loss: 0.7264 - val_acc: 0.7583\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8491 - acc: 0.7084 - val_loss: 0.7081 - val_acc: 0.7623\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8158 - acc: 0.7239 - val_loss: 0.6961 - val_acc: 0.7789\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8101 - acc: 0.7246 - val_loss: 0.6710 - val_acc: 0.7829\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 11s 2ms/step\n",
      ">> * New best model, loss: 0.019\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 41s 6ms/step - loss: 1.8890 - acc: 0.3119 - val_loss: 1.5892 - val_acc: 0.4422\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.5200 - acc: 0.4518 - val_loss: 1.3843 - val_acc: 0.5126\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3917 - acc: 0.4891 - val_loss: 1.2801 - val_acc: 0.5452\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2991 - acc: 0.5296 - val_loss: 1.2040 - val_acc: 0.5727\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.2502 - acc: 0.5425 - val_loss: 1.1150 - val_acc: 0.6008\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.1873 - acc: 0.5727 - val_loss: 1.0455 - val_acc: 0.6266\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1569 - acc: 0.5890 - val_loss: 0.9897 - val_acc: 0.6586\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1076 - acc: 0.6094 - val_loss: 0.9717 - val_acc: 0.6615\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0668 - acc: 0.6217 - val_loss: 0.9236 - val_acc: 0.6798\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0461 - acc: 0.6354 - val_loss: 0.8690 - val_acc: 0.7027\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0129 - acc: 0.6504 - val_loss: 0.8658 - val_acc: 0.6976\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9814 - acc: 0.6567 - val_loss: 0.8241 - val_acc: 0.7199\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9419 - acc: 0.6685 - val_loss: 0.7944 - val_acc: 0.7371\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9207 - acc: 0.6761 - val_loss: 0.7509 - val_acc: 0.7583\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8937 - acc: 0.6944 - val_loss: 0.7399 - val_acc: 0.7635\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8778 - acc: 0.6968 - val_loss: 0.7053 - val_acc: 0.7703\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8605 - acc: 0.7047 - val_loss: 0.6771 - val_acc: 0.7875\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8371 - acc: 0.7149 - val_loss: 0.6851 - val_acc: 0.7795\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8162 - acc: 0.7194 - val_loss: 0.6727 - val_acc: 0.7990\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.7955 - acc: 0.7287 - val_loss: 0.6411 - val_acc: 0.8036\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 12s 2ms/step\n",
      ">> * New best model, loss: 0.032\n",
      ">> Round winner: normalized\n",
      ">>> Finished training tests\n",
      ">>> Round # 4\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 42s 6ms/step - loss: 1.8496 - acc: 0.3350 - val_loss: 1.3785 - val_acc: 0.5241\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.5197 - acc: 0.4473 - val_loss: 1.3021 - val_acc: 0.5246\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.4007 - acc: 0.4948 - val_loss: 1.1967 - val_acc: 0.5641\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3316 - acc: 0.5084 - val_loss: 1.1433 - val_acc: 0.5813\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.2674 - acc: 0.5359 - val_loss: 1.0309 - val_acc: 0.6100\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.2211 - acc: 0.5597 - val_loss: 0.9570 - val_acc: 0.6495\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.1763 - acc: 0.5717 - val_loss: 0.9677 - val_acc: 0.6552\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.1383 - acc: 0.5915 - val_loss: 0.9338 - val_acc: 0.6684\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.1228 - acc: 0.5995 - val_loss: 0.8848 - val_acc: 0.6982\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.0804 - acc: 0.6158 - val_loss: 0.8680 - val_acc: 0.6964\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.0406 - acc: 0.6331 - val_loss: 0.8269 - val_acc: 0.7251\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.0144 - acc: 0.6474 - val_loss: 0.7997 - val_acc: 0.7337\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.9874 - acc: 0.6556 - val_loss: 0.7864 - val_acc: 0.7417\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.9559 - acc: 0.6715 - val_loss: 0.7581 - val_acc: 0.7600\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.9298 - acc: 0.6819 - val_loss: 0.7417 - val_acc: 0.7692\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.9104 - acc: 0.6862 - val_loss: 0.7311 - val_acc: 0.7652\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.8782 - acc: 0.6955 - val_loss: 0.7198 - val_acc: 0.7617\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8833 - acc: 0.6970 - val_loss: 0.6786 - val_acc: 0.7784\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.8379 - acc: 0.7127 - val_loss: 0.6736 - val_acc: 0.7801\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.8309 - acc: 0.7150 - val_loss: 0.6349 - val_acc: 0.8099\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 11s 2ms/step\n",
      ">> * New best model, loss: 0.011\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 41s 6ms/step - loss: 1.8627 - acc: 0.3315 - val_loss: 1.5603 - val_acc: 0.4840\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.5467 - acc: 0.4440 - val_loss: 1.2736 - val_acc: 0.4983\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.4295 - acc: 0.4863 - val_loss: 1.3131 - val_acc: 0.5286\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3177 - acc: 0.5316 - val_loss: 1.2888 - val_acc: 0.5435\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2516 - acc: 0.5485 - val_loss: 1.0812 - val_acc: 0.6060\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.2023 - acc: 0.5709 - val_loss: 1.1023 - val_acc: 0.6082\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1544 - acc: 0.5820 - val_loss: 1.0664 - val_acc: 0.6271\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1199 - acc: 0.5983 - val_loss: 0.9333 - val_acc: 0.6684\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0809 - acc: 0.6142 - val_loss: 0.8738 - val_acc: 0.6821\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0416 - acc: 0.6323 - val_loss: 0.8355 - val_acc: 0.7148\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0196 - acc: 0.6344 - val_loss: 0.7848 - val_acc: 0.7537\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9829 - acc: 0.6533 - val_loss: 0.7452 - val_acc: 0.7583\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9375 - acc: 0.6689 - val_loss: 0.7521 - val_acc: 0.7589\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9305 - acc: 0.6784 - val_loss: 0.7294 - val_acc: 0.7709\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9078 - acc: 0.6828 - val_loss: 0.7281 - val_acc: 0.7669\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8706 - acc: 0.6924 - val_loss: 0.7122 - val_acc: 0.7738\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8481 - acc: 0.7068 - val_loss: 0.7184 - val_acc: 0.7726\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8481 - acc: 0.7021 - val_loss: 0.6856 - val_acc: 0.7812\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8261 - acc: 0.7150 - val_loss: 0.6446 - val_acc: 0.7944\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.7992 - acc: 0.7282 - val_loss: 0.6560 - val_acc: 0.7927\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 11s 2ms/step\n",
      ">> * New best model, loss: 0.004\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 5\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 41s 6ms/step - loss: 1.9125 - acc: 0.3182 - val_loss: 1.5960 - val_acc: 0.4273\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.5319 - acc: 0.4429 - val_loss: 1.4064 - val_acc: 0.4971\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3826 - acc: 0.4986 - val_loss: 1.3315 - val_acc: 0.5435\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3118 - acc: 0.5199 - val_loss: 1.3363 - val_acc: 0.5447\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2336 - acc: 0.5528 - val_loss: 1.1876 - val_acc: 0.5836\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1835 - acc: 0.5703 - val_loss: 1.1094 - val_acc: 0.6123\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1477 - acc: 0.5842 - val_loss: 1.0542 - val_acc: 0.6340\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1062 - acc: 0.6036 - val_loss: 0.9548 - val_acc: 0.6753\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0538 - acc: 0.6231 - val_loss: 0.8968 - val_acc: 0.6970\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0189 - acc: 0.6374 - val_loss: 0.8975 - val_acc: 0.6936\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0017 - acc: 0.6460 - val_loss: 0.8693 - val_acc: 0.7165\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.9491 - acc: 0.6650 - val_loss: 0.8295 - val_acc: 0.7325\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9325 - acc: 0.6723 - val_loss: 0.8140 - val_acc: 0.7377\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9157 - acc: 0.6801 - val_loss: 0.7930 - val_acc: 0.7577\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8893 - acc: 0.6887 - val_loss: 0.7620 - val_acc: 0.7709\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8693 - acc: 0.6938 - val_loss: 0.7470 - val_acc: 0.7715\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8348 - acc: 0.7120 - val_loss: 0.7392 - val_acc: 0.7721\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.7994 - acc: 0.7252 - val_loss: 0.7236 - val_acc: 0.7812\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.7900 - acc: 0.7265 - val_loss: 0.6907 - val_acc: 0.7967\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.7806 - acc: 0.7335 - val_loss: 0.6927 - val_acc: 0.7950\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 12s 2ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 41s 6ms/step - loss: 1.9290 - acc: 0.3128 - val_loss: 1.6925 - val_acc: 0.4164\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.5625 - acc: 0.4372 - val_loss: 1.5186 - val_acc: 0.4737\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.4193 - acc: 0.4891 - val_loss: 1.4765 - val_acc: 0.5103\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.3285 - acc: 0.5302 - val_loss: 1.4212 - val_acc: 0.5040\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.2604 - acc: 0.5477 - val_loss: 1.2870 - val_acc: 0.5641\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.2036 - acc: 0.5769 - val_loss: 1.2007 - val_acc: 0.5962\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.1734 - acc: 0.5826 - val_loss: 1.1298 - val_acc: 0.6220\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.1266 - acc: 0.5969 - val_loss: 1.1396 - val_acc: 0.6249\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0791 - acc: 0.6104 - val_loss: 1.1295 - val_acc: 0.6123\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0404 - acc: 0.6334 - val_loss: 1.0213 - val_acc: 0.6564\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0150 - acc: 0.6436 - val_loss: 0.9732 - val_acc: 0.6770\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9873 - acc: 0.6540 - val_loss: 0.9323 - val_acc: 0.6993\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9530 - acc: 0.6688 - val_loss: 0.9282 - val_acc: 0.7062\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9240 - acc: 0.6818 - val_loss: 0.9311 - val_acc: 0.6867\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.9022 - acc: 0.6857 - val_loss: 0.8660 - val_acc: 0.7142\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8661 - acc: 0.7023 - val_loss: 0.8145 - val_acc: 0.7320\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8586 - acc: 0.7014 - val_loss: 0.7875 - val_acc: 0.7514\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8364 - acc: 0.7156 - val_loss: 0.7575 - val_acc: 0.7566\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8173 - acc: 0.7247 - val_loss: 0.7528 - val_acc: 0.7635\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.7963 - acc: 0.7283 - val_loss: 0.7045 - val_acc: 0.7749\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 11s 2ms/step\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 6\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 42s 6ms/step - loss: 1.8662 - acc: 0.3261 - val_loss: 1.4863 - val_acc: 0.4800\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.5162 - acc: 0.4506 - val_loss: 1.3199 - val_acc: 0.5275\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3805 - acc: 0.4943 - val_loss: 1.2526 - val_acc: 0.5596\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.3292 - acc: 0.5268 - val_loss: 1.1490 - val_acc: 0.5836\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2469 - acc: 0.5510 - val_loss: 1.0733 - val_acc: 0.6002\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1767 - acc: 0.5749 - val_loss: 1.0457 - val_acc: 0.6094\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1402 - acc: 0.5932 - val_loss: 0.9666 - val_acc: 0.6460\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1056 - acc: 0.6081 - val_loss: 0.9412 - val_acc: 0.6598\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0618 - acc: 0.6265 - val_loss: 0.8971 - val_acc: 0.6867\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0380 - acc: 0.6430 - val_loss: 0.8538 - val_acc: 0.6987\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0056 - acc: 0.6480 - val_loss: 0.8203 - val_acc: 0.7131\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9671 - acc: 0.6606 - val_loss: 0.7985 - val_acc: 0.7320\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9492 - acc: 0.6729 - val_loss: 0.7809 - val_acc: 0.7337\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9288 - acc: 0.6731 - val_loss: 0.7805 - val_acc: 0.7371\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9070 - acc: 0.6878 - val_loss: 0.7276 - val_acc: 0.7623\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8751 - acc: 0.7020 - val_loss: 0.7237 - val_acc: 0.7595\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8605 - acc: 0.6983 - val_loss: 0.6964 - val_acc: 0.7824\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8384 - acc: 0.7104 - val_loss: 0.6784 - val_acc: 0.7875\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8049 - acc: 0.7229 - val_loss: 0.6870 - val_acc: 0.7812\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8044 - acc: 0.7236 - val_loss: 0.6738 - val_acc: 0.7847\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 11s 2ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6986/6986 [==============================] - 42s 6ms/step - loss: 1.9040 - acc: 0.3249 - val_loss: 1.7397 - val_acc: 0.4152\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.5623 - acc: 0.4455 - val_loss: 1.3886 - val_acc: 0.5063\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.4587 - acc: 0.4794 - val_loss: 1.3566 - val_acc: 0.5275\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.3658 - acc: 0.5053 - val_loss: 1.4448 - val_acc: 0.4920\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.2946 - acc: 0.5395 - val_loss: 1.4045 - val_acc: 0.5086\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2364 - acc: 0.5544 - val_loss: 1.3048 - val_acc: 0.5395\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1792 - acc: 0.5787 - val_loss: 1.2432 - val_acc: 0.5533\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1337 - acc: 0.5968 - val_loss: 1.1799 - val_acc: 0.5779\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1042 - acc: 0.6091 - val_loss: 1.0554 - val_acc: 0.6203\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0748 - acc: 0.6164 - val_loss: 0.9949 - val_acc: 0.6352\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0425 - acc: 0.6354 - val_loss: 0.9822 - val_acc: 0.6415\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0012 - acc: 0.6466 - val_loss: 0.9039 - val_acc: 0.6638\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9684 - acc: 0.6589 - val_loss: 0.8527 - val_acc: 0.6924\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9525 - acc: 0.6668 - val_loss: 0.8599 - val_acc: 0.6976\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9275 - acc: 0.6743 - val_loss: 0.7998 - val_acc: 0.7194\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9085 - acc: 0.6888 - val_loss: 0.8017 - val_acc: 0.7274\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8717 - acc: 0.6964 - val_loss: 0.8462 - val_acc: 0.7085\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8558 - acc: 0.7053 - val_loss: 0.7525 - val_acc: 0.7560\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8397 - acc: 0.7119 - val_loss: 0.7394 - val_acc: 0.7595\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8152 - acc: 0.7207 - val_loss: 0.7277 - val_acc: 0.7589\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 11s 2ms/step\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 7\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 41s 6ms/step - loss: 1.8426 - acc: 0.3381 - val_loss: 1.6371 - val_acc: 0.4095\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 34s 5ms/step - loss: 1.5361 - acc: 0.4462 - val_loss: 1.5144 - val_acc: 0.4605\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.4112 - acc: 0.4933 - val_loss: 1.4324 - val_acc: 0.4817\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3265 - acc: 0.5155 - val_loss: 1.3411 - val_acc: 0.5103\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2602 - acc: 0.5344 - val_loss: 1.2960 - val_acc: 0.5252\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2072 - acc: 0.5673 - val_loss: 1.2193 - val_acc: 0.5636\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1611 - acc: 0.5809 - val_loss: 1.1491 - val_acc: 0.5865\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1296 - acc: 0.5919 - val_loss: 1.0815 - val_acc: 0.6082\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.0898 - acc: 0.6122 - val_loss: 0.9775 - val_acc: 0.6546\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.0580 - acc: 0.6187 - val_loss: 0.9236 - val_acc: 0.6627\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.0326 - acc: 0.6363 - val_loss: 0.8833 - val_acc: 0.6964\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9855 - acc: 0.6547 - val_loss: 0.8595 - val_acc: 0.7073\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9578 - acc: 0.6638 - val_loss: 0.8281 - val_acc: 0.7113\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.9413 - acc: 0.6711 - val_loss: 0.7921 - val_acc: 0.7405\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.9089 - acc: 0.6891 - val_loss: 0.7771 - val_acc: 0.7400\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8940 - acc: 0.6841 - val_loss: 0.7531 - val_acc: 0.7480\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8813 - acc: 0.6978 - val_loss: 0.7418 - val_acc: 0.7463\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8363 - acc: 0.7200 - val_loss: 0.7070 - val_acc: 0.7658\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8245 - acc: 0.7107 - val_loss: 0.6728 - val_acc: 0.7772\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.8067 - acc: 0.7267 - val_loss: 0.6691 - val_acc: 0.7738\n",
      "1746/1746 [==============================] - 4s 2ms/step\n",
      "6986/6986 [==============================] - 15s 2ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 43s 6ms/step - loss: 1.9426 - acc: 0.3066 - val_loss: 1.7449 - val_acc: 0.4296\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.6209 - acc: 0.4283 - val_loss: 1.4800 - val_acc: 0.4851\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.4940 - acc: 0.4717 - val_loss: 1.3210 - val_acc: 0.5235\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 34s 5ms/step - loss: 1.4216 - acc: 0.4897 - val_loss: 1.2648 - val_acc: 0.5458\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3475 - acc: 0.5273 - val_loss: 1.2050 - val_acc: 0.5882\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2842 - acc: 0.5461 - val_loss: 1.1812 - val_acc: 0.5848\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2198 - acc: 0.5759 - val_loss: 1.2314 - val_acc: 0.5733\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1698 - acc: 0.5829 - val_loss: 1.2700 - val_acc: 0.5601\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.1243 - acc: 0.5996 - val_loss: 1.2571 - val_acc: 0.5630\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0944 - acc: 0.6046 - val_loss: 1.1385 - val_acc: 0.6094\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0380 - acc: 0.6323 - val_loss: 1.0814 - val_acc: 0.6271\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0191 - acc: 0.6434 - val_loss: 0.9561 - val_acc: 0.6649\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9878 - acc: 0.6519 - val_loss: 0.9374 - val_acc: 0.6856\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9680 - acc: 0.6643 - val_loss: 0.9676 - val_acc: 0.6787\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9285 - acc: 0.6749 - val_loss: 0.9128 - val_acc: 0.6838\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9009 - acc: 0.6874 - val_loss: 0.8297 - val_acc: 0.7325\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8824 - acc: 0.6944 - val_loss: 0.7895 - val_acc: 0.7400\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.8587 - acc: 0.7077 - val_loss: 0.8028 - val_acc: 0.7388\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8430 - acc: 0.7113 - val_loss: 0.7607 - val_acc: 0.7497\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8330 - acc: 0.7187 - val_loss: 0.7385 - val_acc: 0.7629\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 11s 2ms/step\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 8\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 43s 6ms/step - loss: 1.9239 - acc: 0.3212 - val_loss: 1.5929 - val_acc: 0.4427\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.5797 - acc: 0.4362 - val_loss: 1.3025 - val_acc: 0.5481\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.4436 - acc: 0.4837 - val_loss: 1.3003 - val_acc: 0.5544\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.3635 - acc: 0.5092 - val_loss: 1.2623 - val_acc: 0.5630\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3081 - acc: 0.5286 - val_loss: 1.2026 - val_acc: 0.5842\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2251 - acc: 0.5574 - val_loss: 1.2157 - val_acc: 0.5808\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1775 - acc: 0.5803 - val_loss: 1.1347 - val_acc: 0.6140\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1287 - acc: 0.5908 - val_loss: 1.0656 - val_acc: 0.6357\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1047 - acc: 0.6036 - val_loss: 1.0687 - val_acc: 0.6306\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0511 - acc: 0.6241 - val_loss: 0.9644 - val_acc: 0.6632\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0233 - acc: 0.6429 - val_loss: 0.9086 - val_acc: 0.6861\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.9781 - acc: 0.6592 - val_loss: 0.9279 - val_acc: 0.6850\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9656 - acc: 0.6662 - val_loss: 0.8792 - val_acc: 0.6942\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9330 - acc: 0.6814 - val_loss: 0.8369 - val_acc: 0.7142\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8907 - acc: 0.6908 - val_loss: 0.8335 - val_acc: 0.7165\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8747 - acc: 0.7031 - val_loss: 0.7985 - val_acc: 0.7331\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8609 - acc: 0.7046 - val_loss: 0.7665 - val_acc: 0.7520\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8331 - acc: 0.7199 - val_loss: 0.7684 - val_acc: 0.7388\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8125 - acc: 0.7212 - val_loss: 0.7502 - val_acc: 0.7589\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8052 - acc: 0.7257 - val_loss: 0.7493 - val_acc: 0.7474\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 11s 2ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 42s 6ms/step - loss: 1.9158 - acc: 0.3229 - val_loss: 1.6030 - val_acc: 0.4341\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.5484 - acc: 0.4526 - val_loss: 1.3978 - val_acc: 0.4885\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 34s 5ms/step - loss: 1.4175 - acc: 0.4928 - val_loss: 1.4368 - val_acc: 0.4977\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 34s 5ms/step - loss: 1.3158 - acc: 0.5322 - val_loss: 1.2445 - val_acc: 0.5447\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 34s 5ms/step - loss: 1.2481 - acc: 0.5548 - val_loss: 1.2143 - val_acc: 0.5578\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.1937 - acc: 0.5743 - val_loss: 1.2511 - val_acc: 0.5493\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1522 - acc: 0.5960 - val_loss: 1.1053 - val_acc: 0.6094\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1056 - acc: 0.6139 - val_loss: 1.0653 - val_acc: 0.6197\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0573 - acc: 0.6364 - val_loss: 1.0504 - val_acc: 0.6352\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0226 - acc: 0.6431 - val_loss: 0.9662 - val_acc: 0.6598\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9957 - acc: 0.6559 - val_loss: 0.9501 - val_acc: 0.6598\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9637 - acc: 0.6675 - val_loss: 0.9371 - val_acc: 0.6730\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9253 - acc: 0.6811 - val_loss: 0.9186 - val_acc: 0.6867\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9099 - acc: 0.6825 - val_loss: 0.8606 - val_acc: 0.7045\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8742 - acc: 0.7014 - val_loss: 0.8268 - val_acc: 0.7205\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.8544 - acc: 0.7183 - val_loss: 0.8059 - val_acc: 0.7302\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8255 - acc: 0.7220 - val_loss: 0.7551 - val_acc: 0.7514\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8030 - acc: 0.7249 - val_loss: 0.7354 - val_acc: 0.7526\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8061 - acc: 0.7234 - val_loss: 0.7303 - val_acc: 0.7560\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.7769 - acc: 0.7373 - val_loss: 0.6976 - val_acc: 0.7686\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 12s 2ms/step\n",
      ">> Round winner: normalized\n",
      ">>> Finished training tests\n",
      ">>> Round # 9\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 43s 6ms/step - loss: 1.8656 - acc: 0.3319 - val_loss: 1.5487 - val_acc: 0.4605\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.5479 - acc: 0.4410 - val_loss: 1.2567 - val_acc: 0.5630\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.4196 - acc: 0.4863 - val_loss: 1.2046 - val_acc: 0.5544\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3496 - acc: 0.5215 - val_loss: 1.1162 - val_acc: 0.5985\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2599 - acc: 0.5470 - val_loss: 1.0328 - val_acc: 0.6306\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.2027 - acc: 0.5673 - val_loss: 1.0458 - val_acc: 0.6191\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1548 - acc: 0.5835 - val_loss: 0.9865 - val_acc: 0.6386\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1139 - acc: 0.6036 - val_loss: 0.9043 - val_acc: 0.6873\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0744 - acc: 0.6190 - val_loss: 0.9131 - val_acc: 0.6901\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0272 - acc: 0.6353 - val_loss: 0.8994 - val_acc: 0.6844\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.0086 - acc: 0.6430 - val_loss: 0.8554 - val_acc: 0.7062\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.9630 - acc: 0.6626 - val_loss: 0.8286 - val_acc: 0.7239\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6986/6986 [==============================] - 35s 5ms/step - loss: 0.9590 - acc: 0.6589 - val_loss: 0.7928 - val_acc: 0.7491\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9271 - acc: 0.6854 - val_loss: 0.7536 - val_acc: 0.7595\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.8926 - acc: 0.6878 - val_loss: 0.7620 - val_acc: 0.7428\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.8768 - acc: 0.6941 - val_loss: 0.7234 - val_acc: 0.7629\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 38s 5ms/step - loss: 0.8471 - acc: 0.7040 - val_loss: 0.7048 - val_acc: 0.7715\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8399 - acc: 0.7081 - val_loss: 0.6568 - val_acc: 0.8030\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8091 - acc: 0.7247 - val_loss: 0.6541 - val_acc: 0.7978\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.7917 - acc: 0.7329 - val_loss: 0.6231 - val_acc: 0.8047\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 11s 2ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 1.9195 - acc: 0.3171 - val_loss: 1.5415 - val_acc: 0.4530\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.5604 - acc: 0.4432 - val_loss: 1.3739 - val_acc: 0.5006\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.4144 - acc: 0.4863 - val_loss: 1.2430 - val_acc: 0.5664\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.3091 - acc: 0.5309 - val_loss: 1.3302 - val_acc: 0.5561\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.2272 - acc: 0.5726 - val_loss: 1.2291 - val_acc: 0.5756\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.1718 - acc: 0.5989 - val_loss: 1.1504 - val_acc: 0.5916\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1249 - acc: 0.6112 - val_loss: 1.1184 - val_acc: 0.6048\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0833 - acc: 0.6204 - val_loss: 1.0219 - val_acc: 0.6334\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.0427 - acc: 0.6290 - val_loss: 0.9259 - val_acc: 0.6747\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0235 - acc: 0.6461 - val_loss: 0.9120 - val_acc: 0.6850\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9726 - acc: 0.6628 - val_loss: 0.8971 - val_acc: 0.6861\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9633 - acc: 0.6623 - val_loss: 0.8352 - val_acc: 0.7159\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9058 - acc: 0.6892 - val_loss: 0.7923 - val_acc: 0.7291\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.9125 - acc: 0.6825 - val_loss: 0.7860 - val_acc: 0.7383\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8773 - acc: 0.6945 - val_loss: 0.8422 - val_acc: 0.7194\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.8565 - acc: 0.7070 - val_loss: 0.7526 - val_acc: 0.7526\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.8271 - acc: 0.7159 - val_loss: 0.7146 - val_acc: 0.7617\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 0.8152 - acc: 0.7260 - val_loss: 0.7067 - val_acc: 0.7732\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.7983 - acc: 0.7219 - val_loss: 0.6759 - val_acc: 0.7835\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 0.7616 - acc: 0.7382 - val_loss: 0.6303 - val_acc: 0.8030\n",
      "1746/1746 [==============================] - 3s 2ms/step\n",
      "6986/6986 [==============================] - 12s 2ms/step\n",
      ">> Round winner: standard\n",
      ">>> Finished training tests\n",
      ">>> Round # 10\n",
      ">> Training with STANDARD data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 43s 6ms/step - loss: 1.8936 - acc: 0.3302 - val_loss: 1.6830 - val_acc: 0.4112\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 35s 5ms/step - loss: 1.5530 - acc: 0.4353 - val_loss: 1.4572 - val_acc: 0.4897\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.4182 - acc: 0.4940 - val_loss: 1.3404 - val_acc: 0.5189\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.3423 - acc: 0.5109 - val_loss: 1.2358 - val_acc: 0.5550\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.2658 - acc: 0.5385 - val_loss: 1.1264 - val_acc: 0.5808\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 37s 5ms/step - loss: 1.2087 - acc: 0.5621 - val_loss: 1.1028 - val_acc: 0.5939\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 39s 6ms/step - loss: 1.1784 - acc: 0.5757 - val_loss: 1.0396 - val_acc: 0.6346\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.1259 - acc: 0.6013 - val_loss: 0.9815 - val_acc: 0.6581\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0929 - acc: 0.6104 - val_loss: 1.0104 - val_acc: 0.6403\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0658 - acc: 0.6191 - val_loss: 0.9475 - val_acc: 0.6747\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 36s 5ms/step - loss: 1.0374 - acc: 0.6358 - val_loss: 0.8898 - val_acc: 0.6970\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 43s 6ms/step - loss: 1.0089 - acc: 0.6516 - val_loss: 0.8771 - val_acc: 0.6924\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 0.9760 - acc: 0.6565 - val_loss: 0.8569 - val_acc: 0.7153\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.9340 - acc: 0.6738 - val_loss: 0.8684 - val_acc: 0.7222\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 0.9236 - acc: 0.6786 - val_loss: 0.8361 - val_acc: 0.7342\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.9057 - acc: 0.6858 - val_loss: 0.7843 - val_acc: 0.7543\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.8786 - acc: 0.7031 - val_loss: 0.7817 - val_acc: 0.7457\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.8579 - acc: 0.7011 - val_loss: 0.7462 - val_acc: 0.7663\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.8258 - acc: 0.7144 - val_loss: 0.7441 - val_acc: 0.7595\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.8133 - acc: 0.7150 - val_loss: 0.6978 - val_acc: 0.7835\n",
      "1746/1746 [==============================] - 4s 2ms/step\n",
      "6986/6986 [==============================] - 15s 2ms/step\n",
      ">> Training with NORMALIZED data...\n",
      "Train on 6986 samples, validate on 1746 samples\n",
      "Epoch 1/20\n",
      "6986/6986 [==============================] - 59s 8ms/step - loss: 1.9420 - acc: 0.3053 - val_loss: 1.6173 - val_acc: 0.4347\n",
      "Epoch 2/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 1.5706 - acc: 0.4319 - val_loss: 1.3444 - val_acc: 0.4983\n",
      "Epoch 3/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 1.4206 - acc: 0.4896 - val_loss: 1.2448 - val_acc: 0.5367\n",
      "Epoch 4/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 1.3304 - acc: 0.5301 - val_loss: 1.2609 - val_acc: 0.5527\n",
      "Epoch 5/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 1.2629 - acc: 0.5528 - val_loss: 1.1429 - val_acc: 0.6054\n",
      "Epoch 6/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 1.1783 - acc: 0.5872 - val_loss: 1.1460 - val_acc: 0.5997\n",
      "Epoch 7/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 1.1514 - acc: 0.5886 - val_loss: 1.1496 - val_acc: 0.6088\n",
      "Epoch 8/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 1.0959 - acc: 0.6134 - val_loss: 1.0781 - val_acc: 0.6438\n",
      "Epoch 9/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 1.0640 - acc: 0.6215 - val_loss: 1.0003 - val_acc: 0.6604\n",
      "Epoch 10/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 1.0391 - acc: 0.6361 - val_loss: 0.9344 - val_acc: 0.6919\n",
      "Epoch 11/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.9986 - acc: 0.6480 - val_loss: 0.9029 - val_acc: 0.6993\n",
      "Epoch 12/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.9790 - acc: 0.6513 - val_loss: 0.8669 - val_acc: 0.7216\n",
      "Epoch 13/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 0.9321 - acc: 0.6769 - val_loss: 0.8239 - val_acc: 0.7320\n",
      "Epoch 14/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 0.9218 - acc: 0.6739 - val_loss: 0.8027 - val_acc: 0.7394\n",
      "Epoch 15/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 0.8814 - acc: 0.6940 - val_loss: 0.8054 - val_acc: 0.7497\n",
      "Epoch 16/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.8835 - acc: 0.7023 - val_loss: 0.8059 - val_acc: 0.7468\n",
      "Epoch 17/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.8606 - acc: 0.7080 - val_loss: 0.7586 - val_acc: 0.7617\n",
      "Epoch 18/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.8250 - acc: 0.7223 - val_loss: 0.7265 - val_acc: 0.7778\n",
      "Epoch 19/20\n",
      "6986/6986 [==============================] - 45s 6ms/step - loss: 0.8146 - acc: 0.7170 - val_loss: 0.7159 - val_acc: 0.7852\n",
      "Epoch 20/20\n",
      "6986/6986 [==============================] - 44s 6ms/step - loss: 0.7931 - acc: 0.7299 - val_loss: 0.6783 - val_acc: 0.7984\n",
      "1746/1746 [==============================] - 4s 2ms/step\n",
      "6986/6986 [==============================] - 16s 2ms/step\n",
      ">> Round winner: normalized\n",
      ">>> Finished training tests\n"
     ]
    }
   ],
   "source": [
    "# Cyclic training settings\n",
    "rounds = 10\n",
    "max_loss_diff = 5.5\n",
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "# Register round results\n",
    "orig_train_losses = []\n",
    "orig_test_losses = []\n",
    "orig_train_accuracies = []\n",
    "orig_test_accuracies = []\n",
    "orig_loss_diffs = []\n",
    "orig_acc_diffs = []\n",
    "norm_train_losses = []\n",
    "norm_test_losses = []\n",
    "norm_train_accuracies = []\n",
    "norm_test_accuracies = []\n",
    "norm_loss_diffs = []\n",
    "norm_acc_diffs = []\n",
    "winners = []\n",
    "\n",
    "# Best models\n",
    "best_orig_overall = 100\n",
    "best_orig_model = None\n",
    "best_orig_history = None\n",
    "best_norm_overall = 100\n",
    "best_norm_model = None\n",
    "best_norm_history = None\n",
    "\n",
    "# Input / Output shapes\n",
    "num_rows = 40\n",
    "num_columns = 174\n",
    "num_channels = 1\n",
    "num_labels = 10\n",
    "input_shape = (num_rows, num_columns, num_channels)\n",
    "\n",
    "# Iterate rounds\n",
    "for idx in range(rounds):\n",
    "\n",
    "    ### \n",
    "    ###   Round setup\n",
    "    ###\n",
    "\n",
    "    # Same randomized set for both data sources\n",
    "    rand_indexes = randomize(num_samples)\n",
    "\n",
    "    # Prepare both matrices: split() returns [X_train, y_train, X_test, y_test]\n",
    "    data_orig = split(X_orig, y_orig, rand_indexes)\n",
    "    data_norm = split(X_norm, y_norm, rand_indexes)\n",
    "\n",
    "\n",
    "    print(\">>> Round #\", idx+1)\n",
    "    \n",
    "\n",
    "    ### \n",
    "    ###   Standard data\n",
    "    ###\n",
    "\n",
    "    print(\">> Training with STANDARD data...\")\n",
    "    \n",
    "    # Create momdel (original dataset)\n",
    "    orig_model = create_model(input_shape, num_labels)\n",
    "\n",
    "    # Train model and fetch results\n",
    "    orig_results = train_model(orig_model,\n",
    "                               data_orig[0], \n",
    "                               data_orig[1], \n",
    "                               data_orig[2], \n",
    "                               data_orig[3], \n",
    "                               num_epochs, \n",
    "                               batch_size)\n",
    "\n",
    "    # Register round results\n",
    "    orig_train_losses.append(orig_results['train_loss'])\n",
    "    orig_test_losses.append(orig_results['test_loss'])\n",
    "    orig_train_accuracies.append(orig_results['train_acc'])\n",
    "    orig_test_accuracies.append(orig_results['test_acc'])\n",
    "    orig_loss_diffs.append(orig_results['loss_diff'])\n",
    "    orig_acc_diffs.append(orig_results['acc_diff'])\n",
    "\n",
    "    # Check best original model\n",
    "    if (orig_results['loss_diff'] < best_orig_overall):\n",
    "        best_orig_overall = orig_results['loss_diff']\n",
    "        best_orig_model = orig_model\n",
    "        best_orig_history = orig_results['history']\n",
    "        print(\">> * New best model, loss: {}\".format(best_orig_overall))\n",
    "\n",
    "\n",
    "\n",
    "    ### \n",
    "    ###   Normalized data\n",
    "    ###\n",
    "\n",
    "    print(\">> Training with NORMALIZED data...\")\n",
    "    \n",
    "    # Create momdel (original dataset)\n",
    "    norm_model = create_model(input_shape, num_labels)\n",
    "\n",
    "    # Train model and fetch results\n",
    "    norm_results = train_model(norm_model,\n",
    "                               data_norm[0], \n",
    "                               data_norm[1], \n",
    "                               data_norm[2], \n",
    "                               data_norm[3], \n",
    "                               num_epochs, \n",
    "                               batch_size)\n",
    "\n",
    "    # Register round results\n",
    "    norm_train_losses.append(norm_results['train_loss'])\n",
    "    norm_test_losses.append(norm_results['test_loss'])\n",
    "    norm_train_accuracies.append(norm_results['train_acc'])\n",
    "    norm_test_accuracies.append(norm_results['test_acc'])\n",
    "    norm_loss_diffs.append(norm_results['loss_diff'])\n",
    "    norm_acc_diffs.append(norm_results['acc_diff'])\n",
    "\n",
    "    # Check best original model\n",
    "    if (norm_results['loss_diff'] < best_norm_overall):\n",
    "        best_norm_overall = norm_results['loss_diff']\n",
    "        best_norm_model = norm_model\n",
    "        best_norm_history = norm_results['history']\n",
    "        print(\">> * New best model, loss: {}\".format(best_norm_overall))\n",
    "\n",
    "    # Register round winner\n",
    "    if ((norm_results['loss_diff'] < max_loss_diff) & (norm_results['test_loss'] < orig_results['test_loss'])):\n",
    "        round_winner = 'normalized'\n",
    "    elif (orig_results['loss_diff'] < max_loss_diff):\n",
    "        round_winner = 'standard'\n",
    "    else:\n",
    "        round_winner = 'none'\n",
    "        \n",
    "    print(\">> Round winner: {}\".format(round_winner))\n",
    "    \n",
    "    # Register winner\n",
    "    winners.append(round_winner)\n",
    "    \n",
    "    print(\">>> Finished training tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_train_loss</th>\n",
       "      <th>norm_train_loss</th>\n",
       "      <th>orig_test_loss</th>\n",
       "      <th>norm_test_loss</th>\n",
       "      <th>orig_train_accuracy</th>\n",
       "      <th>norm_train_accuracy</th>\n",
       "      <th>orig_test_accuracy</th>\n",
       "      <th>norm_test_accuracy</th>\n",
       "      <th>orig_loss_diff</th>\n",
       "      <th>norm_loss_diff</th>\n",
       "      <th>orig_acc_diff</th>\n",
       "      <th>norm_acc_diff</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.670702</td>\n",
       "      <td>0.741416</td>\n",
       "      <td>0.627328</td>\n",
       "      <td>0.693903</td>\n",
       "      <td>79.495991</td>\n",
       "      <td>76.517755</td>\n",
       "      <td>81.563126</td>\n",
       "      <td>78.285142</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.048</td>\n",
       "      <td>2.067</td>\n",
       "      <td>1.767</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.709475</td>\n",
       "      <td>0.702142</td>\n",
       "      <td>0.634842</td>\n",
       "      <td>0.630738</td>\n",
       "      <td>77.892325</td>\n",
       "      <td>76.746850</td>\n",
       "      <td>80.675637</td>\n",
       "      <td>79.630690</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.071</td>\n",
       "      <td>2.783</td>\n",
       "      <td>2.884</td>\n",
       "      <td>normalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.670975</td>\n",
       "      <td>0.641146</td>\n",
       "      <td>0.651739</td>\n",
       "      <td>0.609412</td>\n",
       "      <td>78.293242</td>\n",
       "      <td>80.355097</td>\n",
       "      <td>79.645004</td>\n",
       "      <td>81.248211</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.032</td>\n",
       "      <td>1.352</td>\n",
       "      <td>0.893</td>\n",
       "      <td>normalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.634869</td>\n",
       "      <td>0.655984</td>\n",
       "      <td>0.646202</td>\n",
       "      <td>0.660062</td>\n",
       "      <td>80.985109</td>\n",
       "      <td>79.266896</td>\n",
       "      <td>79.831091</td>\n",
       "      <td>78.800458</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.154</td>\n",
       "      <td>0.466</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.692729</td>\n",
       "      <td>0.704548</td>\n",
       "      <td>0.632137</td>\n",
       "      <td>0.655986</td>\n",
       "      <td>79.495991</td>\n",
       "      <td>77.491409</td>\n",
       "      <td>80.131692</td>\n",
       "      <td>79.373032</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1.882</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.673764</td>\n",
       "      <td>0.727737</td>\n",
       "      <td>0.651732</td>\n",
       "      <td>0.694480</td>\n",
       "      <td>78.465063</td>\n",
       "      <td>75.887743</td>\n",
       "      <td>80.131692</td>\n",
       "      <td>77.139994</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.667</td>\n",
       "      <td>1.252</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.669093</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.615196</td>\n",
       "      <td>0.690921</td>\n",
       "      <td>77.376861</td>\n",
       "      <td>76.288660</td>\n",
       "      <td>80.446608</td>\n",
       "      <td>78.528486</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.048</td>\n",
       "      <td>3.070</td>\n",
       "      <td>2.240</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.749262</td>\n",
       "      <td>0.697644</td>\n",
       "      <td>0.626053</td>\n",
       "      <td>0.588206</td>\n",
       "      <td>74.742268</td>\n",
       "      <td>76.861397</td>\n",
       "      <td>80.647008</td>\n",
       "      <td>81.763527</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.109</td>\n",
       "      <td>5.905</td>\n",
       "      <td>4.902</td>\n",
       "      <td>normalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.623060</td>\n",
       "      <td>0.630253</td>\n",
       "      <td>0.602310</td>\n",
       "      <td>0.604900</td>\n",
       "      <td>80.469645</td>\n",
       "      <td>80.297824</td>\n",
       "      <td>81.691955</td>\n",
       "      <td>81.033496</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.025</td>\n",
       "      <td>1.222</td>\n",
       "      <td>0.736</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.697849</td>\n",
       "      <td>0.678349</td>\n",
       "      <td>0.635288</td>\n",
       "      <td>0.613351</td>\n",
       "      <td>78.350515</td>\n",
       "      <td>79.839633</td>\n",
       "      <td>80.518179</td>\n",
       "      <td>81.291154</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.065</td>\n",
       "      <td>2.168</td>\n",
       "      <td>1.452</td>\n",
       "      <td>normalized</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   orig_train_loss  norm_train_loss  orig_test_loss  norm_test_loss  \\\n",
       "0         0.670702         0.741416        0.627328        0.693903   \n",
       "1         0.709475         0.702142        0.634842        0.630738   \n",
       "2         0.670975         0.641146        0.651739        0.609412   \n",
       "3         0.634869         0.655984        0.646202        0.660062   \n",
       "4         0.692729         0.704548        0.632137        0.655986   \n",
       "5         0.673764         0.727737        0.651732        0.694480   \n",
       "6         0.669093         0.738462        0.615196        0.690921   \n",
       "7         0.749262         0.697644        0.626053        0.588206   \n",
       "8         0.623060         0.630253        0.602310        0.604900   \n",
       "9         0.697849         0.678349        0.635288        0.613351   \n",
       "\n",
       "   orig_train_accuracy  norm_train_accuracy  orig_test_accuracy  \\\n",
       "0            79.495991            76.517755           81.563126   \n",
       "1            77.892325            76.746850           80.675637   \n",
       "2            78.293242            80.355097           79.645004   \n",
       "3            80.985109            79.266896           79.831091   \n",
       "4            79.495991            77.491409           80.131692   \n",
       "5            78.465063            75.887743           80.131692   \n",
       "6            77.376861            76.288660           80.446608   \n",
       "7            74.742268            76.861397           80.647008   \n",
       "8            80.469645            80.297824           81.691955   \n",
       "9            78.350515            79.839633           80.518179   \n",
       "\n",
       "   norm_test_accuracy  orig_loss_diff  norm_loss_diff  orig_acc_diff  \\\n",
       "0           78.285142           0.043           0.048          2.067   \n",
       "1           79.630690           0.075           0.071          2.783   \n",
       "2           81.248211           0.019           0.032          1.352   \n",
       "3           78.800458           0.011           0.004          1.154   \n",
       "4           79.373032           0.061           0.049          0.636   \n",
       "5           77.139994           0.022           0.033          1.667   \n",
       "6           78.528486           0.054           0.048          3.070   \n",
       "7           81.763527           0.123           0.109          5.905   \n",
       "8           81.033496           0.021           0.025          1.222   \n",
       "9           81.291154           0.063           0.065          2.168   \n",
       "\n",
       "   norm_acc_diff      winner  \n",
       "0          1.767    standard  \n",
       "1          2.884  normalized  \n",
       "2          0.893  normalized  \n",
       "3          0.466    standard  \n",
       "4          1.882    standard  \n",
       "5          1.252    standard  \n",
       "6          2.240    standard  \n",
       "7          4.902  normalized  \n",
       "8          0.736    standard  \n",
       "9          1.452  normalized  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save registry\n",
    "registry = pd.DataFrame({\n",
    "    'orig_train_loss': orig_train_losses,\n",
    "    'norm_train_loss': norm_train_losses,\n",
    "    'orig_test_loss': orig_test_losses,\n",
    "    'norm_test_loss': norm_test_losses,\n",
    "    'orig_train_accuracy': orig_train_accuracies,\n",
    "    'norm_train_accuracy': norm_train_accuracies,\n",
    "    'orig_test_accuracy': orig_test_accuracies,\n",
    "    'norm_test_accuracy': norm_test_accuracies,\n",
    "    'orig_loss_diff': orig_loss_diffs,\n",
    "    'norm_loss_diff': norm_loss_diffs,\n",
    "    'orig_acc_diff': orig_acc_diffs,\n",
    "    'norm_acc_diff': norm_acc_diffs,\n",
    "    'winner': winners\n",
    "})\n",
    "\n",
    "registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train/Test loss difference mean\n",
      "Standard: 0.04920000000000001\n",
      "Normalized: 0.048400000000000006\n",
      "\n",
      ">> Train/Test accuracy difference mean\n",
      "Standard: 2.2024\n",
      "Normalized: 1.8474000000000004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\">> Train/Test loss difference mean\")\n",
    "print(\"Standard:\", registry['orig_loss_diff'].mean())\n",
    "print(\"Normalized:\", registry['norm_loss_diff'].mean())\n",
    "\n",
    "print(\"\\n>> Train/Test accuracy difference mean\")\n",
    "print(\"Standard:\", registry['orig_acc_diff'].mean())\n",
    "print(\"Normalized:\", registry['norm_acc_diff'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training history comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test loss comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard history for loss\n",
    "plt.plot(best_orig_history.history['loss'])\n",
    "plt.plot(best_orig_history.history['val_loss'])\n",
    "plt.title('Stadanrd Data - Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Normalized history for loss\n",
    "plt.plot(best_norm_history.history['loss'])\n",
    "plt.plot(best_norm_history.history['val_loss'])\n",
    "plt.title('Normalized Data - Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test accuracy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard history for accuracy\n",
    "plt.plot(best_orig_history.history['acc'])\n",
    "plt.plot(best_orig_history.history['val_acc'])\n",
    "plt.title('Standard Data - Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Standard history for accuracy\n",
    "plt.plot(best_norm_history.history['acc'])\n",
    "plt.plot(best_norm_history.history['val_acc'])\n",
    "plt.title('Normalized Data - Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "After 10 rounds of training with both datasets the results are:\n",
    "\n",
    "* Normalized: 4 wins\n",
    "* Standard: 6 wins\n",
    "\n",
    "The overall train/loss difference among all rounds is very similar, but watching the means we can tell that the models trained with normalized average amplitude data got a slight smaller value. Even though, the difference is very small. Also, the Train/Loss gap is more stable with the standard data, what should be better.\n",
    "\n",
    "Even though, in larger trainings (70+ epochs) I got more overfitted models using the normalized average amplitude data.\n",
    "\n",
    "In one of the many materials I have been following I found more clues that point to the use of NON-normalized audio for classification tasks like the one here:\n",
    "\n",
    "*As for normalization after db-scaling, that seems hit or miss depending on your data. From the paper above, the authors found nearly no difference using various normalization techniques for their data.*\n",
    "\n",
    "Source: https://stackoverflow.com/questions/55513652/which-spectrogram-best-represents-features-of-an-audio-file-for-cnn-based-model/56727927#56727927\n",
    "\n",
    "Related paper: [A Comparison of Audio Signal Preprocessing Methods for Deep Neural Networks on Music Tagging](https://arxiv.org/abs/1709.01922)\n",
    "\n",
    "I may continue experimenting with other type of normalization over the data that do not involve modifying the original audio dynamics. Or, in contrast, experiment augmenting data by random amplitude average alteration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
